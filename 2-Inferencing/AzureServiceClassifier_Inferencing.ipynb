{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with TensorFlow 2.0 on Azure Machine Learning Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Workshop\n",
    "\n",
    "This notebook is Part 2 (Inferencing and Deploying a Model) of a four part workshop that demonstrates an end-to-end workflow for implementing a BERT model using Tensorflow 2.0 on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](2_AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](3_MLOps.md)\n",
    "- Part 4: [Explaining Your Model Interoperability](4_IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "This workshop shows how to convert a TF 2.0 BERT model and deploy the model as Webservice in step-by-step fashion:\n",
    "\n",
    " * Initilize your workspace\n",
    " * Download a previous saved model (saved on Azure Machine Learning)\n",
    " * Test the downloaded model\n",
    " * Display scoring script\n",
    " * Defining an Azure Environment\n",
    " * Deploy Model as Webservice (Local, ACI and AKS)\n",
    " * Test Deployment (Azure ML Service Call, Raw HTTP Request)\n",
    " * Clean up Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n",
    "\n",
    "\n",
    "#### How can we use Azure Machine Learning SDK for deployment and inferencing of a machine learning models?\n",
    "Deployment and inferencing of a machine learning model, is often an cumbersome process. Once you a trained model and a scoring script working on your local machine, you will want to deploy this model as a web service.\n",
    "\n",
    "To facilitate deployment and inferencing, the Azure Machine Learning Python SDK provides a high-level abstraction for model deployment of a web service running on your [local](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#local) machine, in Azure Container Instance ([ACI](https://azure.microsoft.com/en-us/services/container-instances/)) or Azure Kubernetes Service ([AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/)), which allows users to easily deploy their models in the Azure ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) to:\n",
    "    * Install the AML SDK\n",
    "    * Create a workspace and its configuration file (config.json)\n",
    "* For local scoring test, you will also need to have Tensorflow and Keras installed in the current Jupyter kernel.\n",
    "* Please run through Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb) Notebook first to register your model\n",
    "* Make sure you enable [Docker for non-root users](https://docs.docker.com/install/linux/linux-postinstall/) (This is needed to run Local Deployment). Run the following commands in your Terminal and go to the your [Jupyter dashboard](/tree) and click `Quit` on the top right corner. After the shutdown, the Notebook will be automatically refereshed with the new permissions.\n",
    "```bash\n",
    "    sudo usermod -a -G docker $USER\n",
    "    newgrp docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Enable Docker for non-root users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo usermod -a -G docker $USER\n",
    "!newgrp docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Note:** Make you shutdown your Jupyter notebook to enable this access. Go to the your [Jupyter dashboard](/tree) and click `Quit` on the top right corner. After the shutdown, the Notebook will be automatically refereshed with the new permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Service Classification Problem \n",
    "One of the key tasks to ensuring long term success of any Azure service is actively responding to related posts in online forums such as Stackoverflow. In order to keep track of these posts, Microsoft relies on the associated tags to direct questions to the appropriate support team. While Stackoverflow has different tags for each Azure service (azure-web-app-service, azure-virtual-machine-service, etc), people often use the generic **azure** tag. This makes it hard for specific teams to track down issues related to their product and as a result, many questions get left unanswered. \n",
    "\n",
    "**In order to solve this problem, we will be building a model to classify posts on Stackoverflow with the appropriate Azure service tag.**\n",
    "\n",
    "We will be using a BERT (Bidirectional Encoder Representations from Transformers) model which was published by researchers at Google AI Language. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of natural language processing (NLP) tasks without substantial architecture modifications.\n",
    "\n",
    "For more information about the BERT, please read this [paper](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Azure Machine Learning Python SDK Version\n",
    "\n",
    "If you are running this on a Notebook VM, the Azure Machine Learning Python SDK is installed by default. If you are running this locally, you can follow these [instructions](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py) to install it using pip.\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. We can import the Python SDK to ensure it has been properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.69\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the prerequisites step. Workspace.from_config() creates a workspace object from the details stored in config.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: 107151-aml-ws\n",
      "Azure region: westus2\n",
      "Subscription id: edb336ca-b85f-4204-8057-7fdb7d65322c\n",
      "Resource group: aml-rg-107151\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastore\n",
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is used to store connection information to a central data storage. This allows you to access your storage without having to hard code this (potentially confidential) information into your scripts. \n",
    "\n",
    "In this tutorial, the model was been previously prepped and uploaded into a central [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. We will register this container into our workspace as a datastore using a [shared access signature (SAS) token](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview). \n",
    "\n",
    "\n",
    "\n",
    "We need to define the following parameters to register a datastore:\n",
    "\n",
    "- `ws`: The workspace object\n",
    "- `datastore_name`: The name of the datastore, case insensitive, can only contain alphanumeric characters and _.\n",
    "- `container_name`: The name of the azure blob container.\n",
    "- `account_name`: The storage account name.\n",
    "- `sas_token`: An account SAS token, defaults to None.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "datastore_name = 'tfworld'\n",
    "container_name = 'azureml-blobstore-7c6bdd88-21fa-453a-9c80-16998f02935f'\n",
    "account_name = 'tfworld6818510241'\n",
    "sas_token = '?sv=2019-02-02&ss=bfqt&srt=sco&sp=rl&se=2019-11-08T05:12:15Z&st=2019-10-23T20:12:15Z&spr=https&sig=eDqnc51TkqiIklpQfloT5vcU70pgzDuKb5PAGTvCdx4%3D'\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                    datastore_name=datastore_name, \n",
    "                                                    container_name=container_name,\n",
    "                                                    account_name=account_name, \n",
    "                                                    sas_token=sas_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.datastores['tfworld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Model from Datastore\n",
    "Get the trained model from an Azure Blob container. The model is saved into two files, ``config.json`` and ``model.h5``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Path already exists. Skipping download for ./azure-service-classifier/model/bert_tf2.onnx\n",
      "WARNING - Path already exists. Skipping download for ./azure-service-classifier/model/config.json\n",
      "WARNING - Path already exists. Skipping download for ./azure-service-classifier/model/tf_model.h5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "datastore.download('./',prefix=\"azure-service-classifier/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering the Model with the Workspace\n",
    "Register the model to use in your workspace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model azure-service-classifier\n"
     ]
    }
   ],
   "source": [
    "model = Model.register(model_path = \"./azure-service-classifier/model\",\n",
    "                       model_name = \"azure-service-classifier\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"BERT\"},\n",
    "                       workspace = ws)\n",
    "model_dir = './azure-service-classifier/model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Using Registered Models\n",
    "> If you already completed Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb) Notebook.You can dowload your registered BERT Model and use that instead of the model saved on the blob storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model = ws.models['azure-service-classifier']\n",
    "model_dir = model.download(target_dir='.', exist_ok=True, exists_ok=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing on the test set\n",
    "Let's check the version of the local Keras. Make sure it matches with the version number printed out in the training script. Otherwise you might not be able to load the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.3.1\n",
      "Tensorflow version: 2.1.0-dev20191025\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install Transformers Library\n",
    "We have trained BERT model using Tensorflow 2.0 and the open source [huggingface/transformers](https://github.com/huggingface/transformers) libary. So before we can load the model we need to make sure we have also installed the Transformers Library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (2.1.1)\n",
      "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.1.83)\n",
      "Requirement already satisfied: tqdm in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (4.36.1)\n",
      "Requirement already satisfied: sacremoses in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (0.0.35)\n",
      "Requirement already satisfied: boto3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (1.9.250)\n",
      "Requirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: regex in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (2019.8.19)\n",
      "Requirement already satisfied: numpy in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers) (1.16.2)\n",
      "Requirement already satisfied: click in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: six in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers) (1.12.0)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.250 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers) (1.12.250)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers) (0.2.1)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers) (0.9.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.250->boto3->transformers) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.250->boto3->transformers) (0.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Tensorflow 2.0 BERT model.\n",
    "Load the downloaded Tensorflow 2.0 BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "import keras2onnx\n",
    "import onnx\n",
    "from transformers import BertTokenizer, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "class TFBertForMultiClassification(TFBertPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultiClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='softmax')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "    \n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained(model_dir, num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed in test sentence to test the BERT model. And time the duration of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\n",
      "CPU times: user 8.53 s, sys: 78.9 ms, total: 8.61 s\n",
      "Wall time: 8.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json \n",
    "\n",
    "# Input test sentences\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    " # Encode inputs using tokenizer\n",
    "inputs = tokenizer.encode_plus(\n",
    "    json.loads(raw_data)['text'],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_seq_length\n",
    "    )\n",
    "input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "padding_length = max_seq_length - len(input_ids)\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    # Make prediction\n",
    "predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "\n",
    "result =  {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see based on the sample sentence the model can predict the probablity of the stackover flow tags related to that sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX and ONNX Runtime\n",
    "**ONNX (Open Neural Network Exchange)** is an interoperable standard format for ML models, with support for both DNN and traditional ML. Models can be converted from a variety of frameworks, such as Tensorflow, Keras, PyTorch, scikit-learn, and more (see [ONNX Conversion tutorials](https://github.com/onnx/tutorials#converting-to-onnx-format)). This provides data teams with the flexibility to use their framework of choice for their training needs, while streamlining the process to operationalize these models for production usage in a consistent way.\",\n",
    "\n",
    " In this section, we will demonstrate how to use ONNX Runtime, a high performance inference engine for ONNX format models, for inferencing our model. Along with interoperability, ONNX Runtime's performance-focused architecture can also accelerate  inferencing for many models through graph optimizations, utilization of custom accelerators, and more. You can find more about performance tuning [here](https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Perf_Tuning.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can convert to ONNX we need to install the lastest version of Tensorflow 2, due to [bug in the TF2.0 conversion](https://github.com/tensorflow/tensorflow/issues/32849) process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-nightly==2.1.0-dev20191025\n",
      "  Using cached https://files.pythonhosted.org/packages/cc/74/09da6454f37a5d4e4829419ed026cfe4e4199606ec8e2dffafcdf77cbf5c/tf_nightly-2.1.0.dev20191025-cp36-cp36m-manylinux2010_x86_64.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.16.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (0.33.6)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (3.10.0)\n",
      "Requirement already satisfied: tb-nightly<2.2.0a0,>=2.1.0a0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (2.1.0a20191025)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.24.1)\n",
      "Requirement already satisfied: tf-estimator-nightly in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (2.0.0.dev2019102201)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (3.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.12.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (1.0.8)\n",
      "Requirement already satisfied: gast==0.2.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (0.2.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (0.8.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tf-nightly==2.1.0-dev20191025) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.8.0->tf-nightly==2.1.0-dev20191025) (41.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (3.1.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (0.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (1.6.3)\n",
      "Requirement already satisfied: h5py in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tf-nightly==2.1.0-dev20191025) (2.9.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (1.2.0)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (0.2.7)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (3.1.1)\n",
      "Requirement already satisfied: requests>=2.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (2.22.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from rsa>=3.1.4->google-auth<2,>=1.6.3->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (0.4.7)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.2.0a0,>=2.1.0a0->tf-nightly==2.1.0-dev20191025) (1.24.2)\n",
      "Installing collected packages: tf-nightly\n",
      "Successfully installed tf-nightly-2.1.0.dev20191025\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tf-nightly==2.1.0-dev20191025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model = keras2onnx.convert_keras(loaded_model, target_opset=10)\n",
    "onnx.save_model(onnx_model, 'bert_tf2_convert.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download ONNX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - Path already exists. Skipping download for ./azure-service-classifier/model/bert_tf2.onnx\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datastore.download('.',prefix=\"azure-service-classifier/model/bert_tf2.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: onnxruntime in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (0.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install onnxruntime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading ONNX Model\n",
    "Load the downloaded ONNX BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from transformers import BertTokenizer, TFBertPreTrainedModel, TFBertMainLayer\n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "sess = rt.InferenceSession(\"bert_tf2_convert.onnx\")\n",
    "print(\"ONNX Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the inputs and outputs of converted ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name  : token_type_ids:0\n",
      "Input shape : ['unk__847', 128]\n",
      "Input type  : tensor(int32)\n",
      "Input name  : input_ids:0\n",
      "Input shape : ['unk__848', 128]\n",
      "Input type  : tensor(int32)\n",
      "Input name  : attention_mask:0\n",
      "Input shape : ['unk__849', 128]\n",
      "Input type  : tensor(int32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sess.get_inputs())):\n",
    "    input_name = sess.get_inputs()[i].name\n",
    "    print(\"Input name  :\", input_name)\n",
    "    input_shape = sess.get_inputs()[i].shape\n",
    "    print(\"Input shape :\", input_shape)\n",
    "    input_type = sess.get_inputs()[i].type\n",
    "    print(\"Input type  :\", input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output name  : Identity:0\n",
      "Output shape : ['unk__850', 5]\n",
      "Output type  : tensor(float)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sess.get_outputs())):\n",
    "    output_name = sess.get_outputs()[i].name\n",
    "    print(\"Output name  :\", output_name)  \n",
    "    output_shape = sess.get_outputs()[i].shape\n",
    "    print(\"Output shape :\", output_shape)\n",
    "    output_type = sess.get_outputs()[i].type\n",
    "    print(\"Output type  :\", output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencing with ONNX Runtimw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'azure-virtual-machine', 'probability': '0.98652273'}\n",
      "CPU times: user 770 ms, sys: 0 ns, total: 770 ms\n",
      "Wall time: 199 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json \n",
    "\n",
    "# Input test sentences\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "\n",
    "# Encode inputs using tokenizer\n",
    "inputs = tokenizer.encode_plus(\n",
    "    json.loads(raw_data)['text'],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_seq_length\n",
    "    )\n",
    "input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "padding_length = max_seq_length - len(input_ids)\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    # Make prediction\n",
    "convert_input = {\n",
    "        sess.get_inputs()[0].name: np.array(tf.convert_to_tensor([token_type_ids], dtype=tf.int32)),\n",
    "        sess.get_inputs()[1].name: np.array(tf.convert_to_tensor([input_ids], dtype=tf.int32)),\n",
    "        sess.get_inputs()[2].name: np.array(tf.convert_to_tensor([attention_mask], dtype=tf.int32))\n",
    "    }\n",
    "\n",
    "predictions = sess.run([output_name], convert_input)\n",
    "\n",
    "result =  {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy models on Azure ML\n",
    "\n",
    "Now we are ready to deploy the model as a web service running on your [local](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#local) machine, in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/) or Azure Kubernetes Service [AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in. \n",
    "> **Note:** For this Notebook, we'll use the original model format for deployment, but the ONNX model can be deployed in the same way by using ONNX Runtime in the scoring script.\n",
    "\n",
    "![](./images/aml-deploy.png)\n",
    "\n",
    "\n",
    "### Deploying a web service\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service. For this Notebook, we'll use the original model format for deployment, but note that the ONNX model can be deployed in the same way by using ONNX Runtime in the scoring script.\n",
    "\n",
    "To build the correct environment, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the web service\n",
    "* The model you trained before\n",
    "\n",
    "Read more about deployment [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a scoring script that will be invoked by the web service call. We have prepared a [score.py script](code/scoring/score.py) in advance that scores your BERT model.\n",
    "\n",
    "* Note that the scoring script must have two required functions, ``init()`` and ``run(input_data)``.\n",
    "    * In ``init()`` function, you typically load the model into a global object. This function is executed only once when the Docker container is started.\n",
    "    * In ``run(input_data)`` function, the model is used to predict a value based on the input data. The input and output to run typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create and/or use a Conda environment using the [Conda Dependencies object](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.conda_dependencies.condadependencies?view=azure-ml-py) when deploying a Webservice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','pandas'],\n",
    "                                 pip_packages=['numpy','pandas','inference-schema[numpy-support]','azureml-defaults','tensorflow==2.0.0','transformers==2.0.0'])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `myenv.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    "We need to define the [Inference Configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.inferenceconfig?view=azure-ml-py) for the web service. There is support for a source directory, you can upload an entire folder from your local machine as dependencies for the Webservice.\n",
    "Note: in that case, your entry_script and conda_file paths are relative paths to the source_directory path.\n",
    "\n",
    "Sample code for using a source directory:\n",
    "\n",
    "```python\n",
    "inference_config = InferenceConfig(source_directory=\"C:/abc\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   conda_file=\"env/myenv.yml\")\n",
    "```\n",
    "\n",
    " - source_directory = holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - runtime = Which runtime to use for the image. Current supported runtimes are 'spark-py' and 'python\n",
    " - entry_script = contains logic specific to initializing your model and running predictions\n",
    " - conda_file = manages conda and python package dependencies.\n",
    " \n",
    " \n",
    " > **Note:** Deployment uses the inference configuration deployment configuration to deploy the models. The deployment process is similar regardless of the compute target. Deploying to AKS is slightly different because you must provide a reference to the AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=\"./\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"myenv.yml\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as a Local Service\n",
    "\n",
    "Estimated time to complete: **about 3-7 minutes**\n",
    "\n",
    "Configure the image and deploy it locally. The following code goes through these steps:\n",
    "\n",
    "* Build an image on local machine (or VM, if you are using a VM) using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file \n",
    "* Define [Local Deployment Configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.localwebservice?view=azure-ml-py#deploy-configuration-port-none-)\n",
    "* Send the image to local docker instance. \n",
    "* Start up a container using the image.\n",
    "* Get the web service HTTP endpoint.\n",
    "* This has a very quick turnaround time and is great for testing service before it is deployed to production\n",
    "\n",
    "> **Note:** Make sure you enable [Docker for non-root users](https://docs.docker.com/install/linux/linux-postinstall/) (This is needed to run Local Deployment). Run the following commands in your Terminal and go to the your [Jupyter dashboard](/tree) and click `Quit` on the top right corner. After the shutdown, the Notebook will be automatically refereshed with the new permissions.\n",
    "```bash\n",
    "    sudo usermod -a -G docker $USER\n",
    "    newgrp docker\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploy Local Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model azure-service-classifier:7 to /tmp/azureml_h22qxwwv/azure-service-classifier/7\n",
      "Generating Docker build context.\n",
      "2019/10/25 19:18:31 Downloading source code...\n",
      "2019/10/25 19:18:38 Finished downloading source code\n",
      "2019/10/25 19:18:38 Creating Docker network: acb_default_network, driver: 'bridge'\n",
      "2019/10/25 19:18:38 Successfully set up Docker network: acb_default_network\n",
      "2019/10/25 19:18:38 Setting up Docker configuration...\n",
      "2019/10/25 19:18:39 Successfully set up Docker configuration\n",
      "2019/10/25 19:18:39 Logging in to registry: 107151amlws55918bf0.azurecr.io\n",
      "2019/10/25 19:18:41 Successfully logged into 107151amlws55918bf0.azurecr.io\n",
      "2019/10/25 19:18:41 Executing step ID: acb_step_0. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2019/10/25 19:18:41 Scanning for dependencies...\n",
      "2019/10/25 19:18:41 Successfully scanned dependencies\n",
      "2019/10/25 19:18:41 Launching container with name: acb_step_0\n",
      "Sending build context to Docker daemon  59.39kB\n",
      "Step 1/15 : FROM mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04@sha256:a1b514f3ba884b9a7695cbba5638933ddaf222e8ce3e8c81e8cdf861679abb05\n",
      "sha256:a1b514f3ba884b9a7695cbba5638933ddaf222e8ce3e8c81e8cdf861679abb05: Pulling from azureml/base\n",
      "a1298f4ce990: Pulling fs layer\n",
      "04a3282d9c4b: Pulling fs layer\n",
      "9b0d3db6dc03: Pulling fs layer\n",
      "8269c605f3f1: Pulling fs layer\n",
      "6504d449e70c: Pulling fs layer\n",
      "4e38f320d0d4: Pulling fs layer\n",
      "b0a763e8ee03: Pulling fs layer\n",
      "11917a028ca4: Pulling fs layer\n",
      "a6c378d11cbf: Pulling fs layer\n",
      "6cc007ad9140: Pulling fs layer\n",
      "6c1698a608f3: Pulling fs layer\n",
      "8269c605f3f1: Waiting\n",
      "6504d449e70c: Waiting\n",
      "4e38f320d0d4: Waiting\n",
      "b0a763e8ee03: Waiting\n",
      "11917a028ca4: Waiting\n",
      "a6c378d11cbf: Waiting\n",
      "6cc007ad9140: Waiting\n",
      "6c1698a608f3: Waiting\n",
      "9b0d3db6dc03: Verifying Checksum\n",
      "9b0d3db6dc03: Download complete\n",
      "04a3282d9c4b: Verifying Checksum\n",
      "04a3282d9c4b: Download complete\n",
      "8269c605f3f1: Verifying Checksum\n",
      "8269c605f3f1: Download complete\n",
      "a1298f4ce990: Verifying Checksum\n",
      "a1298f4ce990: Download complete\n",
      "4e38f320d0d4: Verifying Checksum\n",
      "4e38f320d0d4: Download complete\n",
      "6504d449e70c: Verifying Checksum\n",
      "6504d449e70c: Download complete\n",
      "b0a763e8ee03: Verifying Checksum\n",
      "b0a763e8ee03: Download complete\n",
      "6cc007ad9140: Download complete\n",
      "11917a028ca4: Verifying Checksum\n",
      "11917a028ca4: Download complete\n",
      "6c1698a608f3: Verifying Checksum\n",
      "6c1698a608f3: Download complete\n",
      "a6c378d11cbf: Verifying Checksum\n",
      "a6c378d11cbf: Download complete\n",
      "a1298f4ce990: Pull complete\n",
      "04a3282d9c4b: Pull complete\n",
      "9b0d3db6dc03: Pull complete\n",
      "8269c605f3f1: Pull complete\n",
      "6504d449e70c: Pull complete\n",
      "4e38f320d0d4: Pull complete\n",
      "b0a763e8ee03: Pull complete\n",
      "11917a028ca4: Pull complete\n",
      "a6c378d11cbf: Pull complete\n",
      "6cc007ad9140: Pull complete\n",
      "6c1698a608f3: Pull complete\n",
      "Digest: sha256:a1b514f3ba884b9a7695cbba5638933ddaf222e8ce3e8c81e8cdf861679abb05\n",
      "Status: Downloaded newer image for mcr.microsoft.com/azureml/base:intelmpi2018.3-ubuntu16.04@sha256:a1b514f3ba884b9a7695cbba5638933ddaf222e8ce3e8c81e8cdf861679abb05\n",
      " ---> 93a72e6bd1ce\n",
      "Step 2/15 : USER root\n",
      " ---> Running in 159a2f8ba4b1\n",
      "Removing intermediate container 159a2f8ba4b1\n",
      " ---> 724981035ed4\n",
      "Step 3/15 : RUN mkdir -p $HOME/.cache\n",
      " ---> Running in 279807f53e30\n",
      "Removing intermediate container 279807f53e30\n",
      " ---> 89c14707999c\n",
      "Step 4/15 : WORKDIR /\n",
      " ---> Running in 79602992e0ff\n",
      "Removing intermediate container 79602992e0ff\n",
      " ---> 62ea92ba8d05\n",
      "Step 5/15 : COPY azureml-environment-setup/99brokenproxy /etc/apt/apt.conf.d/\n",
      " ---> 2911fbc04f9a\n",
      "Step 6/15 : RUN if dpkg --compare-versions `conda --version | grep -oE '[^ ]+$'` lt 4.4.11; then conda install conda==4.4.11; fi\n",
      " ---> Running in 26856d24f7b0\n",
      "Removing intermediate container 26856d24f7b0\n",
      " ---> 7947bd510ecc\n",
      "Step 7/15 : COPY azureml-environment-setup/mutated_conda_dependencies.yml azureml-environment-setup/mutated_conda_dependencies.yml\n",
      " ---> ea259ec80b41\n",
      "Step 8/15 : RUN ldconfig /usr/local/cuda/lib64/stubs && conda env create -p /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe -f azureml-environment-setup/mutated_conda_dependencies.yml && rm -rf \"$HOME/.cache/pip\" && conda clean -aqy && CONDA_ROOT_DIR=$(conda info --root) && rm -rf \"$CONDA_ROOT_DIR/pkgs\" && find \"$CONDA_ROOT_DIR\" -type d -name __pycache__ -exec rm -rf {} + && ldconfig\n",
      " ---> Running in b7d96b640ac3\n",
      "Solving environment: ...working... done\n",
      "\u001b[91m\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.7.12\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "six-1.12.0           | 22 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "libblas-3.8.0        | 10 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "libgfortran-ng-7.3.0 | 1.7 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "openssl-1.0.2r       | 3.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "ca-certificates-2019 | 144 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "wheel-0.33.6         | 35 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "ncurses-5.9          | 1.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "python-3.6.2         | 19.0 MB   | ########## | 100% \u001b[0m\u001b[91m\n",
      "libopenblas-0.3.7    | 7.6 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "certifi-2019.9.11    | 147 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "tk-8.5.19            | 1.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "readline-6.2         | 713 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "_libgcc_mutex-0.1    | 3 KB      | ########## | 100% \u001b[0m\u001b[91m\n",
      "pandas-0.25.2        | 11.4 MB   | ########## | 100% \u001b[0m\u001b[91m\n",
      "sqlite-3.13.0        | 4.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "liblapack-3.8.0      | 10 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "pytz-2019.3          | 237 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "libgcc-ng-9.1.0      | 8.1 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "zlib-1.2.11          | 105 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "libstdcxx-ng-9.1.0   | 4.0 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "pip-19.3.1           | 1.9 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "xz-5.2.4             | 366 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "setuptools-41.4.0    | 624 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "libcblas-3.8.0       | 10 KB     | ########## | 100% \u001b[0m\u001b[91m\n",
      "numpy-1.17.3         | 5.2 MB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "python-dateutil-2.8. | 219 KB    | ########## | 100% \u001b[0m\u001b[91m\n",
      "Downloading and Extracting Packages\n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "Requirement already satisfied: numpy in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from -r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 1)) (1.17.3)\n",
      "Requirement already satisfied: pandas in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from -r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 2)) (0.25.2)\n",
      "Collecting inference-schema[numpy-support]\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/db/54cc017c207e36555db23ac7944e703e8c7c05d6b2e0503041d503af766d/inference_schema-1.0.0-py3-none-any.whl\n",
      "Collecting azureml-defaults==1.0.69.*\n",
      "  Downloading https://files.pythonhosted.org/packages/1c/cb/5b70897d8c98daebcbba9df6a5fd4a5cf80ef52d97fa7ba7c6be5e6a2d69/azureml_defaults-1.0.69-py2.py3-none-any.whl\n",
      "Collecting tensorflow==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
      "Collecting transformers==2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from pandas->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 2)) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from pandas->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 2)) (2.8.0)\n",
      "Collecting wrapt==1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
      "Collecting configparser==3.7.4\n",
      "  Downloading https://files.pythonhosted.org/packages/ba/05/6c96328e92e625fc31445d24d75a2c92ef9ba34fc5b037fe69693c362a0d/configparser-3.7.4-py2.py3-none-any.whl\n",
      "Collecting flask==1.0.3\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/74/670ae9737d14114753b8c8fdf2e8bd212a05d3b361ab15b44937dfd40985/Flask-1.0.3-py2.py3-none-any.whl (92kB)\n",
      "Collecting azureml-core==1.0.69.*\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/4f/b5c71c45f9aa82aa2636dd5ec7e19c6c11138c8ef127faa5cbbbc1efa0b7/azureml_core-1.0.69-py2.py3-none-any.whl (1.1MB)\n",
      "Collecting gunicorn==19.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8c/da/b8dd8deb741bff556db53902d4706774c8e1e67265f69528c14c003644e6/gunicorn-19.9.0-py2.py3-none-any.whl (112kB)\n",
      "Collecting azureml-model-management-sdk==1.0.1b6.post1\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/53/9004a1e7d6d4e796abc4bcc8286bfc2a32739c5fbac3859ca7429a228897/azureml_model_management_sdk-1.0.1b6.post1-py2.py3-none-any.whl (130kB)\n",
      "Collecting json-logging-py==0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/e1/46c70eebf216b830867c4896ee678cb7f1b28bb68a2810c7e9a811cecfbc/json-logging-py-0.2.tar.gz\n",
      "Collecting applicationinsights>=0.11.7\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/53/234c53004f71f0717d8acd37876e0b65c121181167057b9ce1b1795f96a0/applicationinsights-0.11.9-py2.py3-none-any.whl (58kB)\n",
      "Requirement already satisfied: wheel>=0.26 in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from tensorflow==2.0.0->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 5)) (0.33.6)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "Collecting absl-py>=0.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/72/e6e483e2db953c11efa44ee21c5fdb6505c4dffa447b4263ca8af6676b62/absl-py-0.8.1.tar.gz (103kB)\n",
      "Collecting google-pasta>=0.1.6\n",
      "  Downloading https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl (52kB)\n",
      "Collecting protobuf>=3.6.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/52/d8d2dbff74b8bf517c42db8d44c3f9ef6555e6f5d6caddfa3f207b9143df/protobuf-3.10.0-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Collecting astor>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Collecting tensorboard<2.1.0,>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
      "Requirement already satisfied: six>=1.10.0 in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from tensorflow==2.0.0->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 5)) (1.12.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Collecting grpcio>=1.8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading https://files.pythonhosted.org/packages/30/54/c9810421e41ec0bca2228c6f06b1b1189b196b69533cbcac9f71b44727f8/grpcio-1.24.3-cp36-cp36m-manylinux2010_x86_64.whl (2.2MB)\n",
      "Collecting keras-preprocessing>=1.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting requests\n",
      "  Downloading https://files.pythonhosted.org/packages/51/bd/23c926cd341ea6b7dd0b2a00aba99ae0f828be89d72b2190f27c11d4b7fb/requests-2.22.0-py2.py3-none-any.whl (57kB)\n",
      "Collecting sacremoses\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "Collecting boto3\n",
      "  Downloading https://files.pythonhosted.org/packages/0e/41/27fb3969a76240d4c42a8f64b9d5ae78c676bab38e980e03b1bbaef279bd/boto3-1.10.2-py2.py3-none-any.whl (128kB)\n",
      "Collecting regex\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "Collecting tqdm\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/c1/bc1dba38b48f4ae3c4428aea669c5e27bd5a7642a74c8348451e0bd8ff86/tqdm-4.36.1-py2.py3-none-any.whl (52kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "Collecting Werkzeug>=0.14\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/42/3aeda98f96e85fd26180534d36570e4d18108d62ae36f87694b476b83d6f/Werkzeug-0.16.0-py2.py3-none-any.whl (327kB)\n",
      "Collecting Jinja2>=2.10\r\n",
      "  Downloading https://files.pythonhosted.org/packages/65/e0/eb35e762802015cab1ccee04e8a277b03f1d8e53da3ec3106882ec42558b/Jinja2-2.10.3-py2.py3-none-any.whl (125kB)\n",
      "Collecting click>=5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/fa/37/45185cb5abbc30d7257104c434fe0b07e5a195a6847506c074527aa599ec/Click-7.0-py2.py3-none-any.whl (81kB)\n",
      "Collecting itsdangerous>=0.24\n",
      "  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-authorization>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/17/4724694ddb3311955ddc367eddcd0928f8ee2c7b12d5a6f0b12bca0b03db/azure_mgmt_authorization-0.60.0-py2.py3-none-any.whl (82kB)\n",
      "Collecting azure-mgmt-storage>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/8a/d9/117216e5f671f6c3238c50cba583924252c5ee08091a7d10fa1d3113faa3/azure_mgmt_storage-6.0.0-py2.py3-none-any.whl (514kB)\n",
      "Collecting cryptography!=1.9,!=2.0.*,!=2.1.*,!=2.2.*\n",
      "  Downloading https://files.pythonhosted.org/packages/ca/9a/7cece52c46546e214e10811b36b2da52ce1ea7fa203203a629b8dfadad53/cryptography-2.8-cp34-abi3-manylinux2010_x86_64.whl (2.3MB)\n",
      "Collecting SecretStorage\n",
      "  Downloading https://files.pythonhosted.org/packages/82/59/cb226752e20d83598d7fdcabd7819570b0329a61db07cfbdd21b2ef546e3/SecretStorage-3.1.1-py3-none-any.whl\n",
      "Collecting adal>=1.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/b5/3ea9ae3d1096b9ff31e8f1846c47d49f3129a12464ac0a73b602de458298/adal-1.2.2-py2.py3-none-any.whl (53kB)\n",
      "Collecting msrestazure>=0.4.33\n",
      "  Downloading https://files.pythonhosted.org/packages/68/75/5cb56ca8cbc6c5fe476e4878c73f57a331edcf55e5d3fcb4a7377d7d659d/msrestazure-0.6.2-py2.py3-none-any.whl (40kB)\n",
      "Collecting azure-mgmt-resource>=1.2.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/0d/80815326fa04f2a73ea94b0f57c29669c89df5aa5f5e285952f6445a91c4/azure_mgmt_resource-5.1.0-py2.py3-none-any.whl (681kB)\n",
      "Collecting backports.tempfile\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/5c/077f910632476281428fe254807952eb47ca78e720d059a46178c541e669/backports.tempfile-1.0-py2.py3-none-any.whl\n",
      "Collecting azure-mgmt-containerregistry>=2.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/97/70/8c2d0509db466678eba16fa2b0a539499f3b351b1f2993126ad843d5be13/azure_mgmt_containerregistry-2.8.0-py2.py3-none-any.whl (718kB)\n",
      "Collecting docker\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/ca/699d4754a932787ef353a157ada74efd1ceb6d1fc0bfb7989ae1e7b33111/docker-4.1.0-py2.py3-none-any.whl (139kB)\n",
      "Collecting azure-common>=1.1.12\n",
      "  Downloading https://files.pythonhosted.org/packages/00/55/a703923c12cd3172d5c007beda0c1a34342a17a6a72779f8a7c269af0cd6/azure_common-1.1.23-py2.py3-none-any.whl\n",
      "Collecting ruamel.yaml<=0.15.89,>=0.15.35\r\n",
      "  Downloading https://files.pythonhosted.org/packages/36/e1/cc2fa400fa5ffde3efa834ceb15c464075586de05ca3c553753dcd6f1d3b/ruamel.yaml-0.15.89-cp36-cp36m-manylinux1_x86_64.whl (651kB)\n",
      "Collecting pyopenssl\n",
      "  Downloading https://files.pythonhosted.org/packages/01/c8/ceb170d81bd3941cbeb9940fc6cc2ef2ca4288d0ca8929ea4db5905d904d/pyOpenSSL-19.0.0-py2.py3-none-any.whl (53kB)\n",
      "Collecting msrest>=0.5.1\n",
      "  Downloading https://files.pythonhosted.org/packages/27/b0/c34b3ea9b2ed74b800520fbefb312cdb7f05c20b8bd42e5e7662a5614f98/msrest-0.6.10-py2.py3-none-any.whl (82kB)\n",
      "Collecting jmespath\n",
      "  Downloading https://files.pythonhosted.org/packages/83/94/7179c3832a6d45b266ddb2aac329e101367fbdb11f425f13771d27f225bb/jmespath-0.9.4-py2.py3-none-any.whl\n",
      "Collecting azure-graphrbac>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/3e/93/02056aca45162f9fc275d1eaad12a2a07ef92375afb48eabddc4134b8315/azure_graphrbac-0.61.1-py2.py3-none-any.whl (141kB)\n",
      "Collecting pathspec\n",
      "  Downloading https://files.pythonhosted.org/packages/7a/68/5902e8cd7f7b17c5879982a3a3ee2ad0c3b92b80c79989a2d3e1ca8d29e1/pathspec-0.6.0.tar.gz\n",
      "Collecting urllib3>=1.23\n",
      "  Downloading https://files.pythonhosted.org/packages/e0/da/55f51ea951e1b7c63a579c09dd7db825bb730ec1fe9c0180fc77bfb31448/urllib3-1.25.6-py2.py3-none-any.whl (125kB)\n",
      "Collecting azure-mgmt-keyvault>=0.40.0\n",
      "  Downloading https://files.pythonhosted.org/packages/b3/d1/9fed0a3a3b43d0b1ad59599b5c836ccc4cf117e26458075385bafe79575b/azure_mgmt_keyvault-2.0.0-py2.py3-none-any.whl (80kB)\n",
      "Collecting PyJWT\n",
      "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
      "Collecting jsonpickle\n",
      "  Downloading https://files.pythonhosted.org/packages/07/07/c157520a3ebd166c8c24c6ae0ecae7c3968eb4653ff0e5af369bb82f004d/jsonpickle-1.2-py2.py3-none-any.whl\n",
      "Collecting ndg-httpsclient\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/67/c2f508c00ed2a6911541494504b7cac16fe0b0473912568df65fd1801132/ndg_httpsclient-0.5.1-py3-none-any.whl\n",
      "Collecting contextlib2\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Collecting dill>=0.2.7.1\n",
      "  Downloading https://files.pythonhosted.org/packages/c7/11/345f3173809cea7f1a193bfbf02403fff250a3360e0e118a1630985e547d/dill-0.3.1.1.tar.gz (151kB)\n",
      "Collecting liac-arff>=2.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/35/fbc9217cfa91d98888b43e1a19c03a50d716108c58494c558c65e308f372/liac-arff-2.4.0.tar.gz\n",
      "Collecting h5py\n",
      "  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "Requirement already satisfied: setuptools in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 5)) (41.4.0)\r\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/4e/fd492e91abdc2d2fcb70ef453064d980688762079397f779758e055f6575/Markdown-3.1.1-py2.py3-none-any.whl (87kB)\n",
      "Collecting idna<2.9,>=2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/python3.6/site-packages (from requests->transformers==2.0.0->-r /azureml-environment-setup/condaenv.h76upf7n.requirements.txt (line 6)) (2019.9.11)\n",
      "Collecting joblib\n",
      "  Downloading https://files.pythonhosted.org/packages/8f/42/155696f85f344c066e17af287359c9786b436b1bf86029bb3411283274f3/joblib-0.14.0-py2.py3-none-any.whl (294kB)\n",
      "Collecting s3transfer<0.3.0,>=0.2.0\n",
      "  Downloading https://files.pythonhosted.org/packages/16/8a/1fc3dba0c4923c2a76e1ff0d52b305c44606da63f718d14d3231e21c51b0/s3transfer-0.2.1-py2.py3-none-any.whl (70kB)\n",
      "Collecting botocore<1.14.0,>=1.13.2\n",
      "  Downloading https://files.pythonhosted.org/packages/33/57/6cd269b6c243f5409fd60b480bf7b9c98a10fa9bb083af223189d7617db5/botocore-1.13.2-py2.py3-none-any.whl (5.3MB)\n",
      "Collecting MarkupSafe>=0.23\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting cffi!=1.11.3,>=1.8\n",
      "  Downloading https://files.pythonhosted.org/packages/ab/15/d6bd2c322da944ba74ca545dd5e4af6e1e72339cbbc738e6877e349cdfbe/cffi-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (392kB)\n",
      "Collecting jeepney\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/4c/ef880713a6c6d628869596703167eab2edf8e0ec2d870d1089dcb0901b81/jeepney-0.4.1-py3-none-any.whl (60kB)\n",
      "Collecting backports.weakref\n",
      "  Downloading https://files.pythonhosted.org/packages/88/ec/f598b633c3d5ffe267aaada57d961c94fdfa183c5c3ebda2b6d151943db6/backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Collecting websocket-client>=0.32.0\n",
      "  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
      "Collecting isodate>=0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/9f/b36f7774ff5ea8e428fdcfc4bb332c39ee5b9362ddd3d40d9516a55221b2/isodate-0.6.0-py2.py3-none-any.whl (45kB)\n",
      "Collecting requests-oauthlib>=0.5.0\r\n",
      "  Downloading https://files.pythonhosted.org/packages/c2/e2/9fd03d55ffb70fe51f587f20bcf407a6927eb121de86928b34d162f0b1ac/requests_oauthlib-1.2.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1>=0.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/71/8f0d444e3a74e5640a3d5d967c1c6b015da9c655f35b2d308a55d907a517/pyasn1-0.4.7-py2.py3-none-any.whl (76kB)\n",
      "Collecting docutils<0.16,>=0.10\n",
      "  Downloading https://files.pythonhosted.org/packages/22/cd/a6aa959dca619918ccb55023b4cb151949c64d4d5d55b3f4ffd7eee0c6e8/docutils-0.15.2-py3-none-any.whl (547kB)\n",
      "Collecting pycparser\n",
      "  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
      "Building wheels for collected packages: wrapt, json-logging-py, opt-einsum, absl-py, termcolor, gast, sacremoses, pathspec, dill, liac-arff, pycparser\n",
      "  Building wheel for wrapt (setup.py): started\n",
      "  Building wheel for wrapt (setup.py): finished with status 'done'\n",
      "  Created wheel for wrapt: filename=wrapt-1.11.1-cp36-cp36m-linux_x86_64.whl size=66695 sha256=15fba23bc5a1eecf6143c6c8e8561b225d1324ec074fa5c584b7bc15331cfdb6\n",
      "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
      "  Building wheel for json-logging-py (setup.py): started\n",
      "  Building wheel for json-logging-py (setup.py): finished with status 'done'\r\n",
      "  Created wheel for json-logging-py: filename=json_logging_py-0.2-cp36-none-any.whl size=3924 sha256=ca6b35caaeb6c069f693c88fae38aba07a363e0338573a92b5d4985386685070\n",
      "  Stored in directory: /root/.cache/pip/wheels/0d/2e/1c/c638b7589610d8b9358a6e5eb008edacb8b3e9b6d1edc9479f\n",
      "  Building wheel for opt-einsum (setup.py): started\n",
      "  Building wheel for opt-einsum (setup.py): finished with status 'done'\n",
      "  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=61682 sha256=1b76d8282d8a59ac8bea4cf0f09f7fe7c3511f3383844bda5cb750360c633b5e\n",
      "  Stored in directory: /root/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for absl-py (setup.py): started\n",
      "  Building wheel for absl-py (setup.py): finished with status 'done'\n",
      "  Created wheel for absl-py: filename=absl_py-0.8.1-cp36-none-any.whl size=121167 sha256=d3bbc950bd348059708648f21e7b5466f72b1c3d92e6ee87529ec9626d1d0561\n",
      "  Stored in directory: /root/.cache/pip/wheels/a7/15/a0/0a0561549ad11cdc1bc8fa1191a353efd30facf6bfb507aefc\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=b2fe984acebc1cdbd77dabeb8e7a2f6cea5e38a9b00ef840330b1163a063bd72\n",
      "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=b389dd781dcfae17811293d5fa28fd98adae3a1ec84510c39927ef82e05940ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for sacremoses (setup.py): started\n",
      "  Building wheel for sacremoses (setup.py): finished with status 'done'\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=d5b8c7afa300b02d68fdfc6a7e8bf6ddfa16945f2558684ebac474203d3392cd\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "  Building wheel for pathspec (setup.py): started\n",
      "  Building wheel for pathspec (setup.py): finished with status 'done'\n",
      "  Created wheel for pathspec: filename=pathspec-0.6.0-cp36-none-any.whl size=26671 sha256=f4875f2dd79a29359693a226ad2632487fa9334778c0931aa792f7054765821b\n",
      "  Stored in directory: /root/.cache/pip/wheels/62/b8/e1/e2719465b5947c40cd85d613d6cb33449b86a1ca5a6c574269\n",
      "  Building wheel for dill (setup.py): started\n",
      "  Building wheel for dill (setup.py): finished with status 'done'\n",
      "  Created wheel for dill: filename=dill-0.3.1.1-cp36-none-any.whl size=78532 sha256=b5f2755503b554f7b5fc8233fb96751c3d043dda7cdb2e9bae1ebf8d7fac8d08\n",
      "  Stored in directory: /root/.cache/pip/wheels/59/b1/91/f02e76c732915c4015ab4010f3015469866c1eb9b14058d8e7\n",
      "  Building wheel for liac-arff (setup.py): started\n",
      "  Building wheel for liac-arff (setup.py): finished with status 'done'\n",
      "  Created wheel for liac-arff: filename=liac_arff-2.4.0-cp36-none-any.whl size=13333 sha256=a4cb2005479210b31faf650c0bdfbe3739b3fa808d6765d8b8b525d76066bc2d\n",
      "  Stored in directory: /root/.cache/pip/wheels/d1/6a/e7/529dc54d76ecede4346164a09ae3168df358945612710f5203\n",
      "  Building wheel for pycparser (setup.py): started\n",
      "  Building wheel for pycparser (setup.py): finished with status 'done'\n",
      "  Created wheel for pycparser: filename=pycparser-2.19-py2.py3-none-any.whl size=111029 sha256=5d2eb2ae506604092268b64cb1a8ad17e2df480023e788591c00043a6aab399b\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
      "Successfully built wrapt json-logging-py opt-einsum absl-py termcolor gast sacremoses pathspec dill liac-arff pycparser\n",
      "Installing collected packages: wrapt, inference-schema, configparser, Werkzeug, MarkupSafe, Jinja2, click, itsdangerous, flask, idna, chardet, urllib3, requests, isodate, oauthlib, requests-oauthlib, msrest, azure-common, pycparser, cffi, cryptography, PyJWT, adal, msrestazure, azure-mgmt-authorization, azure-mgmt-storage, jeepney, SecretStorage, azure-mgmt-resource, backports.weakref, backports.tempfile, azure-mgmt-containerregistry, websocket-client, docker, ruamel.yaml, pyopenssl, jmespath, azure-graphrbac, pathspec, azure-mgmt-keyvault, jsonpickle, pyasn1, ndg-httpsclient, contextlib2, azureml-core, gunicorn, dill, liac-arff, azureml-model-management-sdk, json-logging-py, applicationinsights, azureml-defaults, h5py, keras-applications, opt-einsum, absl-py, google-pasta, protobuf, tensorflow-estimator, termcolor, astor, grpcio, markdown, tensorboard, gast, keras-preprocessing, tensorflow, joblib, tqdm, sacremoses, docutils, botocore, s3transfer, boto3, regex, sentencepiece, transformers\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed Jinja2-2.10.3 MarkupSafe-1.1.1 PyJWT-1.7.1 SecretStorage-3.1.1 Werkzeug-0.16.0 absl-py-0.8.1 adal-1.2.2 applicationinsights-0.11.9 astor-0.8.0 azure-common-1.1.23 azure-graphrbac-0.61.1 azure-mgmt-authorization-0.60.0 azure-mgmt-containerregistry-2.8.0 azure-mgmt-keyvault-2.0.0 azure-mgmt-resource-5.1.0 azure-mgmt-storage-6.0.0 azureml-core-1.0.69 azureml-defaults-1.0.69 azureml-model-management-sdk-1.0.1b6.post1 backports.tempfile-1.0 backports.weakref-1.0.post1 boto3-1.10.2 botocore-1.13.2 cffi-1.13.1 chardet-3.0.4 click-7.0 configparser-3.7.4 contextlib2-0.6.0.post1 cryptography-2.8 dill-0.3.1.1 docker-4.1.0 docutils-0.15.2 flask-1.0.3 gast-0.2.2 google-pasta-0.1.7 grpcio-1.24.3 gunicorn-19.9.0 h5py-2.10.0 idna-2.8 inference-schema-1.0.0 isodate-0.6.0 itsdangerous-1.1.0 jeepney-0.4.1 jmespath-0.9.4 joblib-0.14.0 json-logging-py-0.2 jsonpickle-1.2 keras-applications-1.0.8 keras-preprocessing-1.1.0 liac-arff-2.4.0 markdown-3.1.1 msrest-0.6.10 msrestazure-0.6.2 ndg-httpsclient-0.5.1 oauthlib-3.1.0 opt-einsum-3.1.0 pathspec-0.6.0 protobuf-3.10.0 pyasn1-0.4.7 pycparser-2.19 pyopenssl-19.0.0 regex-2019.8.19 requests-2.22.0 requests-oauthlib-1.2.0 ruamel.yaml-0.15.89 s3transfer-0.2.1 sacremoses-0.0.35 sentencepiece-0.1.83 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.1 termcolor-1.1.0 tqdm-4.36.1 transformers-2.0.0 urllib3-1.25.6 websocket-client-0.56.0 wrapt-1.11.1\n",
      "\u001b[91m\n",
      "\u001b[0m#\n",
      "# To activate this environment, use:\n",
      "# > source activate /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe\n",
      "#\n",
      "# To deactivate an active environment, use:\n",
      "# > source deactivate\n",
      "#\n",
      "\n",
      "Removing intermediate container b7d96b640ac3\n",
      " ---> 34254d228e18\n",
      "Step 9/15 : ENV PATH /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/bin:$PATH\n",
      " ---> Running in bbc0a9f7122c\n",
      "Removing intermediate container bbc0a9f7122c\n",
      " ---> 87ca279968c3\n",
      "Step 10/15 : ENV AZUREML_CONDA_ENVIRONMENT_PATH /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe\n",
      " ---> Running in 89a64fd34ae8\n",
      "Removing intermediate container 89a64fd34ae8\n",
      " ---> 466e00146c24\n",
      "Step 11/15 : ENV LD_LIBRARY_PATH /azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib:$LD_LIBRARY_PATH\n",
      " ---> Running in ea715813fe10\n",
      "Removing intermediate container ea715813fe10\n",
      " ---> c45ae1fb8f9e\n",
      "Step 12/15 : COPY azureml-environment-setup/spark_cache.py azureml-environment-setup/log4j.properties /azureml-environment-setup/\n",
      " ---> e8a61dfc8335\n",
      "Step 13/15 : RUN if [ $SPARK_HOME ]; then /bin/bash -c '$SPARK_HOME/bin/spark-submit  /azureml-environment-setup/spark_cache.py'; fi\n",
      " ---> Running in a69bcf2c4190\n",
      "Removing intermediate container a69bcf2c4190\n",
      " ---> df297f649228\n",
      "Step 14/15 : ENV AZUREML_ENVIRONMENT_IMAGE True\n",
      " ---> Running in 3770a072dd71\n",
      "Removing intermediate container 3770a072dd71\n",
      " ---> 8447a9e44d13\n",
      "Step 15/15 : CMD [\"bash\"]\n",
      " ---> Running in 2a200b13126a\n",
      "Removing intermediate container 2a200b13126a\n",
      " ---> 169a981cfbd0\n",
      "Successfully built 169a981cfbd0\n",
      "Successfully tagged 107151amlws55918bf0.azurecr.io/azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3:latest\n",
      "2019/10/25 19:21:45 Successfully executed container: acb_step_0\n",
      "2019/10/25 19:21:45 Executing step ID: acb_step_1. Timeout(sec): 5400, Working directory: '', Network: 'acb_default_network'\n",
      "2019/10/25 19:21:45 Pushing image: 107151amlws55918bf0.azurecr.io/azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3:latest, attempt 1\n",
      "The push refers to repository [107151amlws55918bf0.azurecr.io/azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3]\n",
      "2e03e380ffea: Preparing\n",
      "2204c69f9a88: Preparing\n",
      "a160847b2936: Preparing\n",
      "6c1f739fee45: Preparing\n",
      "630e5344e779: Preparing\n",
      "6d60e1e5ecec: Preparing\n",
      "e1171d4d60ca: Preparing\n",
      "6ef1a8ae63b7: Preparing\n",
      "85389f9ead9e: Preparing\n",
      "f2608f66a0e3: Preparing\n",
      "0e259b09e5f4: Preparing\n",
      "340dc32eb998: Preparing\n",
      "df18b66efaa6: Preparing\n",
      "ccdb13a20bf2: Preparing\n",
      "9513cdf4e497: Preparing\n",
      "7f083f9454c0: Preparing\n",
      "29f36b5893dc: Preparing\n",
      "6d60e1e5ecec: Waiting\n",
      "e1171d4d60ca: Waiting\n",
      "6ef1a8ae63b7: Waiting\n",
      "85389f9ead9e: Waiting\n",
      "f2608f66a0e3: Waiting\n",
      "0e259b09e5f4: Waiting\n",
      "340dc32eb998: Waiting\n",
      "df18b66efaa6: Waiting\n",
      "ccdb13a20bf2: Waiting\n",
      "9513cdf4e497: Waiting\n",
      "7f083f9454c0: Waiting\n",
      "29f36b5893dc: Waiting\n",
      "630e5344e779: Pushed\n",
      "6c1f739fee45: Pushed\n",
      "2e03e380ffea: Pushed\n",
      "a160847b2936: Pushed\n",
      "6d60e1e5ecec: Pushed\n",
      "e1171d4d60ca: Pushed\n",
      "6ef1a8ae63b7: Pushed\n",
      "340dc32eb998: Pushed\n",
      "0e259b09e5f4: Pushed\n",
      "f2608f66a0e3: Pushed\n",
      "ccdb13a20bf2: Pushed\n",
      "85389f9ead9e: Pushed\n",
      "9513cdf4e497: Pushed\n",
      "7f083f9454c0: Pushed\n",
      "29f36b5893dc: Pushed\n",
      "df18b66efaa6: Pushed\n",
      "2204c69f9a88: Pushed\n",
      "latest: digest: sha256:3287f8710cf3f5504d55d64cb6a874282d051ea72a0058de7d0f8fc3e219ed44 size: 3883\n",
      "2019/10/25 19:23:18 Successfully pushed image: 107151amlws55918bf0.azurecr.io/azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3:latest\n",
      "2019/10/25 19:23:18 Step ID: acb_step_0 marked as successful (elapsed time in seconds: 184.183962)\n",
      "2019/10/25 19:23:18 Populating digests for step ID: acb_step_0...\n",
      "2019/10/25 19:23:19 Successfully populated digests for step ID: acb_step_0\n",
      "2019/10/25 19:23:19 Step ID: acb_step_1 marked as successful (elapsed time in seconds: 92.805871)\n",
      "2019/10/25 19:23:19 The following dependencies were found:\n",
      "2019/10/25 19:23:19 \n",
      "- image:\n",
      "    registry: 107151amlws55918bf0.azurecr.io\n",
      "    repository: azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3\n",
      "    tag: latest\n",
      "    digest: sha256:3287f8710cf3f5504d55d64cb6a874282d051ea72a0058de7d0f8fc3e219ed44\n",
      "  runtime-dependency:\n",
      "    registry: mcr.microsoft.com\n",
      "    repository: azureml/base\n",
      "    tag: intelmpi2018.3-ubuntu16.04\n",
      "    digest: sha256:a1b514f3ba884b9a7695cbba5638933ddaf222e8ce3e8c81e8cdf861679abb05\n",
      "  git: {}\n",
      "\n",
      "\n",
      "Run ID: cc1 was successful after 4m49s\n",
      "Package creation Succeeded\n",
      "Logging into Docker registry 107151amlws55918bf0.azurecr.io\n",
      "Logging into Docker registry 107151amlws55918bf0.azurecr.io\n",
      "Building Docker image from Dockerfile...\n",
      "Step 1/5 : FROM 107151amlws55918bf0.azurecr.io/azureml/azureml_c1f241d01b5197c539f2c3744f68f5e3\n",
      " ---> 169a981cfbd0\n",
      "Step 2/5 : COPY azureml-app /var/azureml-app\n",
      " ---> ad72483cf97b\n",
      "Step 3/5 : COPY model_config_map.json /var/azureml-app/model_config_map.json\n",
      " ---> 1e87db3540e1\n",
      "Step 4/5 : RUN mv '/var/azureml-app/tmprfq0oe0z.py' /var/azureml-app/main.py\n",
      " ---> Running in 74b7e8536011\n",
      " ---> ad9bf6214112\n",
      "Step 5/5 : CMD [\"runsvdir\",\"/var/runit\"]\n",
      " ---> Running in 07276780f991\n",
      " ---> b7f825b7b974\n",
      "Successfully built b7f825b7b974\n",
      "Successfully tagged mymodel:latest\n",
      "Starting Docker container...\n",
      "Docker container running.\n",
      "Checking container health...\n",
      "Local webservice is running at http://localhost:32770\n",
      "32770\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "# Create a local deployment for the web service endpoint\n",
    "deployment_config = LocalWebservice.deploy_configuration()\n",
    "# Deploy the service\n",
    "local_service = Model.deploy(\n",
    "    ws, \"mymodel\", [model], inference_config, deployment_config)\n",
    "# Wait for the deployment to complete\n",
    "local_service.wait_for_deployment(True)\n",
    "# Display the port that the web service is available on\n",
    "print(local_service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:32770/score\n"
     ]
    }
   ],
   "source": [
    "print(local_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the deployed model. Pick a random samples about an issue, and send it to the web service. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a scoring call...\n",
      "Scoring result:\n",
      "{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\n",
      "CPU times: user 8.76 ms, sys: 80 µs, total: 8.84 ms\n",
      "Wall time: 9.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = local_service.run(input_data=raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading Webservice\n",
    "You can update your score.py file and then call reload() to quickly restart the service. This will only reload your execution script and dependency files, it will not rebuild the underlying Docker image. As a result, reload() is fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "class TFBertForMultiClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultiClassification, self) \\\n",
    "            .__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            config.num_labels,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            name='classifier',\n",
    "            activation='softmax')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(\n",
    "            pooled_output,\n",
    "            training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # add hidden states and attention if they are here\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage',\n",
    "    'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "\n",
    "\n",
    "def init():\n",
    "    global tokenizer, model\n",
    "    # os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'azure-service-classifier')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "    model_dir = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model')\n",
    "    model = TFBertForMultiClassification \\\n",
    "        .from_pretrained(model_dir, num_labels=len(labels))\n",
    "    print(\"hello from the reloaded script\")\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        json.loads(raw_data)['text'],\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens.\n",
    "    # Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor(\n",
    "            [attention_mask],\n",
    "            dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor(\n",
    "            [token_type_ids], \n",
    "            dtype=tf.int32)\n",
    "    })\n",
    "\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "init()\n",
    "run(json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container has been successfully cleaned up.\n",
      "Starting Docker container...\n",
      "Docker container running.\n"
     ]
    }
   ],
   "source": [
    "local_service.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating Webservice\n",
    "If you do need to rebuild the image -- to add a new Conda or pip package, for instance -- you will have to call update(), instead (see below).\n",
    "\n",
    "```python\n",
    "local_service.update(models=[loaded_model], \n",
    "                     image_config=None, \n",
    "                     deployment_config=None, \n",
    "                     wait=False, inference_config=None)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    ">**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell\n",
    "\n",
    "You should see the phrase **\"hello from the reloaded script\"** in the logs, because we added it to the script when we did a service reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '2019-10-25T19:40:01,073342513+00:00 - rsyslog/run \\n'\n",
      " '2019-10-25T19:40:01,073431413+00:00 - gunicorn/run \\n'\n",
      " '2019-10-25T19:40:01,073570613+00:00 - iot-server/run \\n'\n",
      " 'bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by bash)\\n'\n",
      " '2019-10-25T19:40:01,079037511+00:00 - nginx/run \\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libcrypto.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libcrypto.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " 'EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '2019-10-25T19:40:01,385214780+00:00 - iot-server/finish 1 0\\n'\n",
      " '2019-10-25T19:40:01,386507180+00:00 - Exit code 1 is normal. Not restarting '\n",
      " 'iot-server.\\n'\n",
      " 'Starting gunicorn 19.9.0\\n'\n",
      " 'Listening at: http://127.0.0.1:31311 (12)\\n'\n",
      " 'Using worker: sync\\n'\n",
      " 'worker timeout is set to 300\\n'\n",
      " 'Booting worker with pid: 43\\n'\n",
      " 'Initialized PySpark session.\\n'\n",
      " 'TensorFlow version 2.0.0 available.\\n'\n",
      " 'To use data.metrics please install scikit-learn. See '\n",
      " 'https://scikit-learn.org/stable/index.html\\n'\n",
      " 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt '\n",
      " 'not found in cache or force_download set to True, downloading to '\n",
      " '/tmp/tmppwqciud8\\n'\n",
      " 'copying /tmp/tmppwqciud8 to cache at '\n",
      " '/root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\\n'\n",
      " 'creating metadata file for '\n",
      " '/root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\\n'\n",
      " 'removing temp file /tmp/tmppwqciud8\\n'\n",
      " 'loading configuration file '\n",
      " 'azureml-models/azure-service-classifier/7/model/config.json\\n'\n",
      " 'Model config {\\n'\n",
      " '  \"attention_probs_dropout_prob\": 0.1,\\n'\n",
      " '  \"finetuning_task\": null,\\n'\n",
      " '  \"hidden_act\": \"gelu\",\\n'\n",
      " '  \"hidden_dropout_prob\": 0.1,\\n'\n",
      " '  \"hidden_size\": 768,\\n'\n",
      " '  \"initializer_range\": 0.02,\\n'\n",
      " '  \"intermediate_size\": 3072,\\n'\n",
      " '  \"layer_norm_eps\": 1e-12,\\n'\n",
      " '  \"max_position_embeddings\": 512,\\n'\n",
      " '  \"num_attention_heads\": 12,\\n'\n",
      " '  \"num_hidden_layers\": 12,\\n'\n",
      " '  \"num_labels\": 5,\\n'\n",
      " '  \"output_attentions\": false,\\n'\n",
      " '  \"output_hidden_states\": false,\\n'\n",
      " '  \"pruned_heads\": {},\\n'\n",
      " '  \"torchscript\": false,\\n'\n",
      " '  \"type_vocab_size\": 2,\\n'\n",
      " '  \"use_bfloat16\": false,\\n'\n",
      " '  \"vocab_size\": 28996\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'loading weights file '\n",
      " 'azureml-models/azure-service-classifier/7/model/tf_model.h5\\n'\n",
      " '\\r'\n",
      " '  0%|          | 0/213450 [00:00<?, ?B/s]\\r'\n",
      " ' 24%|██▍       | 52224/213450 [00:00<00:00, 405868.80B/s]\\r'\n",
      " '100%|██████████| 213450/213450 [00:00<00:00, 1086338.28B/s]2019-10-25 '\n",
      " '19:40:05.297663: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your '\n",
      " 'CPU supports instructions that this TensorFlow binary was not compiled to '\n",
      " 'use: AVX2 FMA\\n'\n",
      " '2019-10-25 19:40:05.303263: I '\n",
      " 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: '\n",
      " '2294680000 Hz\\n'\n",
      " '2019-10-25 19:40:05.303598: I '\n",
      " 'tensorflow/compiler/xla/service/service.cc:168] XLA service 0x4e4f790 '\n",
      " 'executing computations on platform Host. Devices:\\n'\n",
      " '2019-10-25 19:40:05.303643: I '\n",
      " 'tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): '\n",
      " 'Host, Default Version\\n'\n",
      " '2019-10-25 19:40:05.314326: W '\n",
      " 'tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 '\n",
      " 'exceeds 10% of system memory.\\n'\n",
      " '2019-10-25 19:40:05.501942: W '\n",
      " 'tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 '\n",
      " 'exceeds 10% of system memory.\\n'\n",
      " '2019-10-25 19:40:05.521971: W '\n",
      " 'tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 '\n",
      " 'exceeds 10% of system memory.\\n'\n",
      " '2019-10-25 19:40:10.172903: W '\n",
      " 'tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 '\n",
      " 'exceeds 10% of system memory.\\n'\n",
      " '2019-10-25 19:40:10.239080: W '\n",
      " 'tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 89075712 '\n",
      " 'exceeds 10% of system memory.\\n'\n",
      " 'hello from the reloaded script\\n'\n",
      " \"{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\\n\"\n",
      " 'Initializing logger\\n'\n",
      " 'Starting up app insights client\\n'\n",
      " 'Starting up request id generator\\n'\n",
      " 'Starting up app insight hooks\\n'\n",
      " \"Invoking user's init function\\n\"\n",
      " 'loading configuration file '\n",
      " 'azureml-models/azure-service-classifier/7/model/config.json\\n'\n",
      " 'Model config {\\n'\n",
      " '  \"attention_probs_dropout_prob\": 0.1,\\n'\n",
      " '  \"finetuning_task\": null,\\n'\n",
      " '  \"hidden_act\": \"gelu\",\\n'\n",
      " '  \"hidden_dropout_prob\": 0.1,\\n'\n",
      " '  \"hidden_size\": 768,\\n'\n",
      " '  \"initializer_range\": 0.02,\\n'\n",
      " '  \"intermediate_size\": 3072,\\n'\n",
      " '  \"layer_norm_eps\": 1e-12,\\n'\n",
      " '  \"max_position_embeddings\": 512,\\n'\n",
      " '  \"num_attention_heads\": 12,\\n'\n",
      " '  \"num_hidden_layers\": 12,\\n'\n",
      " '  \"num_labels\": 5,\\n'\n",
      " '  \"output_attentions\": false,\\n'\n",
      " '  \"output_hidden_states\": false,\\n'\n",
      " '  \"pruned_heads\": {},\\n'\n",
      " '  \"torchscript\": false,\\n'\n",
      " '  \"type_vocab_size\": 2,\\n'\n",
      " '  \"use_bfloat16\": false,\\n'\n",
      " '  \"vocab_size\": 28996\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'loading weights file '\n",
      " 'azureml-models/azure-service-classifier/7/model/tf_model.h5\\n'\n",
      " 'hello from the reloaded script\\n'\n",
      " \"Users's init has completed successfully\\n\"\n",
      " '\\n'\n",
      " 'Scoring timeout setting is not found. Use default timeout: 3600000 ms\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy in ACI\n",
    "Estimated time to complete: **about 3-7 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "* Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "* Define [ACI Deployment Configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.aciwebservice?view=azure-ml-py#deploy-configuration-cpu-cores-none--memory-gb-none--tags-none--properties-none--description-none--location-none--auth-enabled-none--ssl-enabled-none--enable-app-insights-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--ssl-cname-none-)\n",
    "* Send the image to the ACI container.\n",
    "* Start up a container in ACI using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "## Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. \n",
    "## If you feel you need more later, you would have to recreate the image and redeploy the service.\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=2, \n",
    "                                               memory_gb=4, \n",
    "                                               tags={\"model\": \"BERT\",  \"method\" : \"tensorflow\"}, \n",
    "                                               description='Predict StackoverFlow tags with BERT')\n",
    "\n",
    "aci_service_name = 'asc-aciservice'\n",
    "\n",
    "try:\n",
    "    # if you want to get existing service below is the command\n",
    "    # since aci name needs to be unique in subscription deleting existing aci if any\n",
    "    # we use aci_service_name to create azure ac\n",
    "    aci_service = Webservice(ws, name=aci_service_name)\n",
    "    if aci_service:\n",
    "        aci_service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://78bc80a2-7f3e-4fa4-879f-acaec24f3296.westus2.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the deployed model. Pick a random samples about an Azure issue, and send it to the web service. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\n",
      "CPU times: user 24.2 ms, sys: 1.08 ms, total: 25.3 ms\n",
      "Wall time: 44.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = aci_service.run(input_data=raw_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    ">**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2019-10-25T20:11:57,227785474+00:00 - rsyslog/run \\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " 'bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by bash)\\n'\n",
      " '2019-10-25T20:11:57,229030771+00:00 - iot-server/run \\n'\n",
      " '2019-10-25T20:11:57,230760367+00:00 - gunicorn/run \\n'\n",
      " '2019-10-25T20:11:57,246894727+00:00 - nginx/run \\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libcrypto.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libcrypto.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " '/usr/sbin/nginx: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libssl.so.1.0.0: '\n",
      " 'no version information available (required by /usr/sbin/nginx)\\n'\n",
      " 'EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\\n'\n",
      " '/bin/bash: '\n",
      " '/azureml-envs/azureml_acb89c589d9589e8a1eaf57a0759fafe/lib/libtinfo.so.5: no '\n",
      " 'version information available (required by /bin/bash)\\n'\n",
      " '2019-10-25T20:11:57,318027252+00:00 - iot-server/finish 1 0\\n'\n",
      " '2019-10-25T20:11:57,319267549+00:00 - Exit code 1 is normal. Not restarting '\n",
      " 'iot-server.\\n'\n",
      " 'Starting gunicorn 19.9.0\\n'\n",
      " 'Listening at: http://127.0.0.1:31311 (12)\\n'\n",
      " 'Using worker: sync\\n'\n",
      " 'worker timeout is set to 300\\n'\n",
      " 'Booting worker with pid: 40\\n'\n",
      " 'Initialized PySpark session.\\n'\n",
      " 'TensorFlow version 2.0.0 available.\\n'\n",
      " 'To use data.metrics please install scikit-learn. See '\n",
      " 'https://scikit-learn.org/stable/index.html\\n'\n",
      " 'https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt '\n",
      " 'not found in cache or force_download set to True, downloading to '\n",
      " '/tmp/tmpplkhdvsq\\n'\n",
      " 'copying /tmp/tmpplkhdvsq to cache at '\n",
      " '/root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\\n'\n",
      " 'creating metadata file for '\n",
      " '/root/.cache/torch/transformers/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\\n'\n",
      " 'removing temp file /tmp/tmpplkhdvsq\\n'\n",
      " 'loading configuration file '\n",
      " 'azureml-models/azure-service-classifier/7/model/config.json\\n'\n",
      " 'Model config {\\n'\n",
      " '  \"attention_probs_dropout_prob\": 0.1,\\n'\n",
      " '  \"finetuning_task\": null,\\n'\n",
      " '  \"hidden_act\": \"gelu\",\\n'\n",
      " '  \"hidden_dropout_prob\": 0.1,\\n'\n",
      " '  \"hidden_size\": 768,\\n'\n",
      " '  \"initializer_range\": 0.02,\\n'\n",
      " '  \"intermediate_size\": 3072,\\n'\n",
      " '  \"layer_norm_eps\": 1e-12,\\n'\n",
      " '  \"max_position_embeddings\": 512,\\n'\n",
      " '  \"num_attention_heads\": 12,\\n'\n",
      " '  \"num_hidden_layers\": 12,\\n'\n",
      " '  \"num_labels\": 5,\\n'\n",
      " '  \"output_attentions\": false,\\n'\n",
      " '  \"output_hidden_states\": false,\\n'\n",
      " '  \"pruned_heads\": {},\\n'\n",
      " '  \"torchscript\": false,\\n'\n",
      " '  \"type_vocab_size\": 2,\\n'\n",
      " '  \"use_bfloat16\": false,\\n'\n",
      " '  \"vocab_size\": 28996\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'loading weights file '\n",
      " 'azureml-models/azure-service-classifier/7/model/tf_model.h5\\n'\n",
      " '\\r'\n",
      " '  0%|          | 0/213450 [00:00<?, ?B/s]\\r'\n",
      " ' 24%|██▍       | 52224/213450 [00:00<00:00, 410143.58B/s]\\r'\n",
      " '100%|██████████| 213450/213450 [00:00<00:00, 1092937.38B/s]2019-10-25 '\n",
      " '20:12:01.007379: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your '\n",
      " 'CPU supports instructions that this TensorFlow binary was not compiled to '\n",
      " 'use: AVX2 FMA\\n'\n",
      " '2019-10-25 20:12:01.013475: I '\n",
      " 'tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: '\n",
      " '2394455000 Hz\\n'\n",
      " '2019-10-25 20:12:01.013783: I '\n",
      " 'tensorflow/compiler/xla/service/service.cc:168] XLA service 0x481d2f0 '\n",
      " 'executing computations on platform Host. Devices:\\n'\n",
      " '2019-10-25 20:12:01.013813: I '\n",
      " 'tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): '\n",
      " 'Host, Default Version\\n'\n",
      " 'hello from the reloaded script\\n'\n",
      " \"{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\\n\"\n",
      " 'Initializing logger\\n'\n",
      " 'Starting up app insights client\\n'\n",
      " 'Starting up request id generator\\n'\n",
      " 'Starting up app insight hooks\\n'\n",
      " \"Invoking user's init function\\n\"\n",
      " 'loading configuration file '\n",
      " 'azureml-models/azure-service-classifier/7/model/config.json\\n'\n",
      " 'Model config {\\n'\n",
      " '  \"attention_probs_dropout_prob\": 0.1,\\n'\n",
      " '  \"finetuning_task\": null,\\n'\n",
      " '  \"hidden_act\": \"gelu\",\\n'\n",
      " '  \"hidden_dropout_prob\": 0.1,\\n'\n",
      " '  \"hidden_size\": 768,\\n'\n",
      " '  \"initializer_range\": 0.02,\\n'\n",
      " '  \"intermediate_size\": 3072,\\n'\n",
      " '  \"layer_norm_eps\": 1e-12,\\n'\n",
      " '  \"max_position_embeddings\": 512,\\n'\n",
      " '  \"num_attention_heads\": 12,\\n'\n",
      " '  \"num_hidden_layers\": 12,\\n'\n",
      " '  \"num_labels\": 5,\\n'\n",
      " '  \"output_attentions\": false,\\n'\n",
      " '  \"output_hidden_states\": false,\\n'\n",
      " '  \"pruned_heads\": {},\\n'\n",
      " '  \"torchscript\": false,\\n'\n",
      " '  \"type_vocab_size\": 2,\\n'\n",
      " '  \"use_bfloat16\": false,\\n'\n",
      " '  \"vocab_size\": 28996\\n'\n",
      " '}\\n'\n",
      " '\\n'\n",
      " 'loading weights file '\n",
      " 'azureml-models/azure-service-classifier/7/model/tf_model.h5\\n'\n",
      " 'hello from the reloaded script\\n'\n",
      " \"Users's init has completed successfully\\n\"\n",
      " '\\n'\n",
      " 'Scoring timeout setting is not found. Use default timeout: 3600000 ms\\n'\n",
      " 'Validation Request Content-Type\\n'\n",
      " 'Received input: {\"text\": \"My VM is not working\"}\\n'\n",
      " 'Headers passed in (total 11):\\n'\n",
      " '\\tHost: localhost:5001\\n'\n",
      " '\\tX-Real-Ip: 127.0.0.1\\n'\n",
      " '\\tX-Forwarded-For: 127.0.0.1\\n'\n",
      " '\\tX-Forwarded-Proto: http\\n'\n",
      " '\\tConnection: close\\n'\n",
      " '\\tContent-Length: 32\\n'\n",
      " '\\tUser-Agent: python-requests/2.22.0\\n'\n",
      " '\\tAccept: */*\\n'\n",
      " '\\tAccept-Encoding: gzip, deflate\\n'\n",
      " '\\tContent-Type: application/json\\n'\n",
      " '\\tX-Ms-Request-Id: 8ccc2929-0ca2-44b4-aec2-4a1d5d184f43\\n'\n",
      " 'Scoring Timer is set to 3600.0 seconds\\n'\n",
      " \"{'prediction': 'azure-virtual-machine', 'probability': '0.98652285'}\\n\"\n",
      " '200\\n'\n",
      " '127.0.0.1 - - [25/Oct/2019:20:13:04 +0000] \"POST /score HTTP/1.0\" 200 68 \"-\" '\n",
      " '\"python-requests/2.22.0\"\\n')\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(aci_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy in AKS (Single Node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time to complete: **about 15-25 minutes**, 10-15 mins for AKS provisioning and 5-10 mins to deploy service\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "* Provision a Dev Test AKS Cluster\n",
    "* Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "* Define [AKS Provisioning Configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.akscompute?view=azure-ml-py#provisioning-configuration-agent-count-none--vm-size-none--ssl-cname-none--ssl-cert-pem-file-none--ssl-key-pem-file-none--location-none--vnet-resourcegroup-name-none--vnet-name-none--subnet-name-none--service-cidr-none--dns-service-ip-none--docker-bridge-cidr-none--cluster-purpose-none-)\n",
    "* Provision an AKS Cluster\n",
    "* Define [AKS Deployment Configuration](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.webservice.akswebservice?view=azure-ml-py#deploy-configuration-autoscale-enabled-none--autoscale-min-replicas-none--autoscale-max-replicas-none--autoscale-refresh-seconds-none--autoscale-target-utilization-none--collect-model-data-none--auth-enabled-none--cpu-cores-none--memory-gb-none--enable-app-insights-none--scoring-timeout-ms-none--replica-max-concurrent-requests-none--max-request-wait-time-none--num-replicas-none--primary-key-none--secondary-key-none--tags-none--properties-none--description-none--gpu-cores-none--period-seconds-none--initial-delay-seconds-none--timeout-seconds-none--success-threshold-none--failure-threshold-none--namespace-none--token-auth-enabled-none-)\n",
    "* Send the image to the AKS cluster.\n",
    "* Start up a container in AKS using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Use the default configuration (you can also provide parameters to customize this).\n",
    "# For example, to create a dev/test cluster, use:\n",
    "# prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "\n",
    "aks_name = 'myaks'\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aks_target = AksCompute(ws,\"myaks\")\n",
    "\n",
    "## Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your cluster. \n",
    "## If you feel you need more later, you would have to recreate the image and redeploy the service.\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 2, memory_gb = 4)\n",
    "\n",
    "aks_service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config, aks_target)\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Azure SDK service call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Azure SDK to make a service call with a simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = aks_service.run(input_data=raw_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aks_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HTTP call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will make a Jupyter widget so we can now send construct raw HTTP request and send to the service through the widget."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Web Service with HTTP call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, VBox\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a query',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Get Tag!\")\n",
    "output = widgets.Output()\n",
    "\n",
    "items = [text, button] \n",
    "\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='70%')\n",
    "\n",
    "box_auto = Box(children=items, layout=box_layout)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        input_data = '{\\\"text\\\": \\\"'+ text.value +'\\\"}'\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        resp = requests.post(local_service.scoring_uri, input_data, headers=headers)\n",
    "       \n",
    "        print(\"=\"*10)\n",
    "        print(\"Question:\", text.value)\n",
    "        print(\"POST to url\", local_service.scoring_uri)\n",
    "        print(\"Prediction:\", resp.text)\n",
    "        print(\"=\"*10)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "#Display the GUI\n",
    "VBox([box_auto, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing a raw HTTP request and send to the service through without a widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'My VM is not working'\n",
    "input_data = '{\\\"text\\\": \\\"'+ query +'\\\"}'\n",
    "headers = {'Content-Type':'application/json'}\n",
    "resp = requests.post(local_service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"=\"*10)\n",
    "print(\"Question:\", query)\n",
    "print(\"POST to url\", local_service.scoring_uri)\n",
    "print(\"Prediction:\", resp.text)\n",
    "print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    ">**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of workspace\n",
    "Let's look at the workspace after the web service was deployed. You should see\n",
    "\n",
    "* a registered model named and with the id \n",
    "* an AKS and ACI webservice called with some scoring URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, model in models.items():\n",
    "    print(\"Model: {}, ID: {}\".format(name, model.id))\n",
    "    \n",
    "webservices = ws.webservices\n",
    "for name, webservice in webservices.items():\n",
    "    print(\"Webservice: {}, scoring URI: {}\".format(name, webservice.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete ACI to clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.delete()\n",
    "aci_service.delete()\n",
    "aks_service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
