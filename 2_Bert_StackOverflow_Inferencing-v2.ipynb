{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with TensorFlow 2.0 and ONNX on Azure Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial shows how to convert Tensorflow 2.0 BERT model to ONNX and deploy on Azure Machine Learning. BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
    "\n",
    "For more information about the BERT, please read this [paper](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Workshop\n",
    "\n",
    "This workshop shows how to convert a TF 2.0 BERT model, convert to ONNX and deploy the model as Webservice in step-by-step fashion:\n",
    "\n",
    " 1. Initilize your workspace\n",
    " 2. Download a previous saved model (saved on Azure Machine Learning)\n",
    " 3. Test the downloaded model\n",
    " 4. Convert TF 2.0 model to ONNX\n",
    " 5. Register ONNX model\n",
    " 6. Create a scoring script\n",
    " 7. Defining an Azure Environment\n",
    " 8. Deploy Model as Webservice\n",
    " 9. Test Deployment\n",
    " 10. Clean up Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the **architecture and terms**(add link) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the **configuration notebook**(add link) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (config.json)\n",
    "* For local scoring test, you will also need to have tensorflow and keras installed in the current Jupyter kernel.\n",
    "\n",
    "* Run through [1_Bert_StackOverflow_Training](1_Bert_StackOverflow_Training.ipynb) Notebook first to register your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. Workspace.from_config() creates a workspace object from the details stored in config.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the saved model\n",
    "You can dowload your BERT Model created in the first notebook. [1_Bert_StackOverflow_Training](1_Bert_StackOverflow_Training.ipynb) Notebook. The model is saved into two files, ``model.json`` and ``model.h5``. Azure ML automatically uploaded and is associated with the registered model. We can use the model object to download the model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model = model = ws.models['bert-stackoverflow-v2-local']\n",
    "model.download(target_dir='.', exist_ok=False, exists_ok=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set\n",
    "Let's check the version of the local Keras. Make sure it matches with the version number printed out in the training script. Otherwise you might not be able to load the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the BERT model\n",
    "Load the downloaded BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "model_dir = \"./exports\"\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(model_dir, num_labels=len(labels), force_download=True)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed in test sentence to test the BERT model. And time the duration of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json \n",
    "\n",
    "# Input test sentences\n",
    "raw_data = json.dumps({\n",
    "    'text': 'I need help with importing a module with tensorflow 2.0'\n",
    "})\n",
    "\n",
    "text = json.loads(raw_data)['text']\n",
    "inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='tf')\n",
    "predictions = loaded_model.predict(inputs)\n",
    "\n",
    "#Map labels to the predictions\n",
    "results = zip(labels,predictions[0])\n",
    "for prediction in results:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see based on the sample sentence the model can predict the probablity of the stackover flow tags related to that sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert To ONNX\n",
    "\n",
    "Lets convert our TF 2.0 model to ONNX. ONNX is an open format to represent deep learning models. With ONNX, AI developers can more easily move models between state-of-the-art tools and choose the combination that is best for them. ONNX is developed and supported by a community of partners.\n",
    "\n",
    "Read more about ONNX [here](https://onnx.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core import Model\n",
    "import numpy as np\n",
    "#import keras2onnx\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertForSequenceClassification\n",
    "#import onnxruntime\n",
    "labels = ['c#', '.net', 'java', 'asp.net', 'c++', 'javascript', 'php', 'python', 'sql', 'sql-server'] # should use blob storage location\n",
    "\n",
    "# loaded_model = TFBertForSequenceClassification.from_pretrained(os.path.join('./exports/'), num_labels=len(labels), force_download=True)\n",
    "\n",
    "# # convert to onnx model\n",
    "# onnx_model = keras2onnx.convert_keras(loaded_model, model.name)\n",
    "# onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register ONNX Model\n",
    "Register an existing trained model, add descirption and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register the model\n",
    "from azureml.core.model import Model\n",
    "model_onnx = Model.register(model_path = \"bert-stackoverflow-onnx.onnx\", # this points to a local file\n",
    "                       model_name = \"bert-stackoverflow-onnx\", # this is the name the model is registered as\n",
    "                       tags={\"model\": \"BERT\",  \"method\" : \"tensorflow\"},\n",
    "                       description='BERT multilabel model for tagging stackoverflow posts v2.',\n",
    "                       workspace = ws)\n",
    "\n",
    "print(model_onnx.name, model_onnx.description, model_onnx.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Now we are ready to deploy the model as a web service running in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in.\n",
    "\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service hosted in ACI. \n",
    "\n",
    "To build the correct environment for ACI, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the ACI\n",
    "* The model you trained before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a scoring script that will be invoked by the web service call.\n",
    "\n",
    "* Note that the scoring script must have two required functions, init() and run(input_data).\n",
    "    * In init() function, you typically load the model into a global object. This function is executed only once when the Docker container is started.\n",
    "    * In run(input_data) function, the model is used to predict a value based on the input data. The input and output to run typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from azureml.core import Model\n",
    "\n",
    "labels = ['c#', '.net', 'java', 'asp.net', 'c++', 'javascript', 'php', 'python', 'sql', 'sql-server']\n",
    "\n",
    "def init():\n",
    "    global tokenizer, model\n",
    "    model_dir = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'exports')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = TFBertForSequenceClassification.from_pretrained(model_dir, num_labels=len(labels))\n",
    "\n",
    "def run(raw_data):\n",
    "    text = json.loads(raw_data)['text']\n",
    "    inputs = tokenizer.encode_plus(text, add_special_tokens=True, return_tensors='tf')\n",
    "    predictions = model.predict(inputs)\n",
    "    print(predictions)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create and/or use an Environment object when deploying a Webservice. The Environment can have been previously registered with your Workspace, or it will be registered with it as a part of the Webservice deployment. Only Environments that were created using azureml-defaults version 1.0.48 or later will work with this new handling however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','pandas'],\n",
    "                                 pip_packages=['numpy','pandas','inference-schema[numpy-support]','azureml-defaults','tensorflow==2.0.0','transformers==2.0.0'])\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `myenv.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"myenv.yml\",\"r\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create configuration file\n",
    "\n",
    "Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. While it depends on your model, the default of 1 core and 1 gigabyte of RAM is usually sufficient for many models. If you feel you need more later, you would have to recreate the image and redeploy the service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n",
    "                                               memory_gb=1, \n",
    "                                               tags={\"model\": \"BERT\",  \"method\" : \"tensorflow\"}, \n",
    "                                               description='Predict StackoverFlow tags with BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    "There is now support for a source directory, you can upload an entire folder from your local machine as dependencies for the Webservice.\n",
    "Note: in that case, your entry_script, conda_file, and extra_docker_file_steps paths are relative paths to the source_directory path.\n",
    "\n",
    "Sample code for using a source directory:\n",
    "\n",
    "```python\n",
    "inference_config = InferenceConfig(source_directory=\"C:/abc\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   conda_file=\"env/myenv.yml\", \n",
    "                                   extra_docker_file_steps=\"helloworld.txt\")\n",
    "```\n",
    "\n",
    " - source_directory = holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - runtime = Which runtime to use for the image. Current supported runtimes are 'spark-py' and 'python\n",
    " - entry_script = contains logic specific to initializing your model and running predictions\n",
    " - conda_file = manages conda and python package dependencies.\n",
    " - extra_docker_file_steps = optional: any extra steps you want to inject into docker file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=\".\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"../myenv.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy in ACI\n",
    "Estimated time to complete: **about 5-10 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "* Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "* Register that image under the workspace. \n",
    "* Send the image to the ACI container.\n",
    "* Start up a container in ACI using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "\n",
    "aci_service_name = 'bert-stackoverflow-aciservice-onnx'\n",
    "\n",
    "try:\n",
    "    # if you want to get existing service below is the command\n",
    "    # since aci name needs to be unique in subscription deleting existing aci if any\n",
    "    # we use aci_service_name to create azure ac\n",
    "    service = Webservice(ws, name=aci_service_name)\n",
    "    if service:\n",
    "        service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "\n",
    "service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "\n",
    "service.wait_for_deployment(True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the deployed model. Pick a random samples about Tenserflow 2.0, and send it to the web service hosted in ACI. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "test_sample = json.dumps({\n",
    "    'text': 'I need help with importing a module with tensorflow 2.0'\n",
    "})\n",
    "\n",
    "#test_sample_encoded = bytes(test_sample, encoding='utf8')\n",
    "prediction = service.run(input_data=test_sample)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HTTP call\n",
    "\n",
    "We can retreive the API keys used for accessing the HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive the API keys. two keys were generated.\n",
    "key1, Key2 = service.get_keys()\n",
    "print(key1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send construct raw HTTP request and send to the service. Don't forget to add key to the HTTP header."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# send a random row from the test set to score\n",
    "random_index = np.random.randint(0, len(X_test)-1)\n",
    "input_data = \"{\\\"text\\\": \\\"I need help with importing a module with tensorflow 2.0 \\\"}\"\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization': 'Bearer ' + key1}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "\n",
    "print(\"POST to url\", service.scoring_uri)\n",
    "print(\"prediction:\", resp.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of workspace\n",
    "Let's look at the workspace after the web service was deployed. You should see\n",
    "\n",
    "* a registered model named 'bert-stackoverflow-onnx' and with the id 'bert-stackoverflow-onnx:1'\n",
    "* a webservice called 'bert-stackoverflow-aciservice-onnx' with some scoring URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, model in models.items():\n",
    "    print(\"Model: {}, ID: {}\".format(name, model.id))\n",
    "    \n",
    "webservices = ws.webservices\n",
    "for name, webservice in webservices.items():\n",
    "    print(\"Webservice: {}, scoring URI: {}\".format(name, webservice.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View ACI Logs (Debug, when something goes wrong )\n",
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete ACI to clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
