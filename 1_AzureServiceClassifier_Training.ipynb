{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training With Tensorflow 2.0 On Azure Machine Learning Service\n",
    "\n",
    "## Overview of Tutorial\n",
    "This notebook is Part 1 (Working With Data and Training) of a four part workshop that demonstrates an end-to-end workflow using Tensorflow 2.0 on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Working With Data and Training](https://github.com/johnwu0604/azure-service-classifier)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/johnwu0604/azure-service-classifier)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/johnwu0604/azure-service-classifier)\n",
    "- Part 4: [Explaining Your Model Interoperability](https://github.com/johnwu0604/azure-service-classifier)\n",
    "\n",
    "**In this specific tutorial, we will cover the following topics:**\n",
    "\n",
    "- Introduction to Azure Machine Learning service and its resources\n",
    "- Preparing training data and uploading it to a central Blob storage\n",
    "- Registering datastore and datasets to a workspace\n",
    "- Creating a remote compute target and training a model on it\n",
    "- Registering the trained model for future deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n",
    "\n",
    "\n",
    "#### How can we use it for training machine learning models?\n",
    "Training machine learning models, particularly deep neural networks, is often a time- and compute-intensive task. Once you've finished writing your training script and running on a small subset of data on your local machine, you will likely want to scale up your workload.\n",
    "\n",
    "To facilitate training, the Azure Machine Learning Python SDK provides a high-level abstraction, the estimator class, which allows users to easily train their models in the Azure ecosystem. You can create and use an Estimator object to submit any training code you want to run on remote compute, whether it's a single-node run or distributed training across a GPU cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Service Classification Problem \n",
    "One of the key tasks to ensuring long term success of any Azure service is actively responding to related posts in online forums such as Stackoverflow. In order to keep track of these posts, Microsoft relies on the associated tags to direct questions to the appropriate support team. While Stackoverflow has different tags for each Azure service (azure-web-app-service, azure-virtual-machine-service, etc), people often use the generic **azure** tag. This makes it hard for specific teams to track down issues related to their product and as a result, many questions get left unanswered. \n",
    "\n",
    "**In order to solve this problem, we will be building a model to classify posts on Stackoverflow with the appropriate Azure service tag.**\n",
    "\n",
    "We will be using a BERT (Bidirectional Encoder Representations from Transformers) model which was published by researchers at Google AI Language. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of natural language processing (NLP) tasks without substantial architecture modifications.\n",
    "\n",
    "For more information about the BERT, please read this [paper](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data Using Databricks\n",
    "\n",
    "TODO:\n",
    "- Explain how we went from raw stackoverflow data to a processed form\n",
    "- Include spark/pandas source code that is reproducible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Azure Machine Learning Python SDK\n",
    "\n",
    "If you are running this on a Notebook VM, the Azure Machine Learning Python SDK is installed by default. If you are running this locally, you can follow these [instructions](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py) to install it using pip.\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. We can import the Python SDK to ensure it has been properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. The workspace holds all your experiments, compute targets, models, datastores, etc.\n",
    "\n",
    "You can [click here](https://ml.azure.com) to access your workspace resources through a graphical user interface.\n",
    "\n",
    "![](./images/aml-workspace.png)\n",
    "\n",
    "The following code will allow you to create a workspace if you don't already have one created. You must have an Azure subscription to create a workspace:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2')\n",
    "```\n",
    "\n",
    "**If you are running this on a Notebook VM, you can import the existing workspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that the above commands reads a config.json file that exists by default within the Notebook VM. If you are running this locally or want to use a different workspace, you must add a config file to your project directory. The config file should have the following schema:\n",
    "\n",
    "```\n",
    "    {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION-ID>\",\n",
    "        \"resource_group\": \"<RESOURCE-GROUP>\",\n",
    "        \"workspace_name\": \"<WORKSPACE-NAME>\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Compute Target\n",
    "\n",
    "A [compute target](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.computetarget?view=azure-ml-py) is a designated compute resource/environment where you run your training script or host your service deployment. This location may be your local machine or a cloud-based compute resource. Compute targets can be reused across the workspace for different runs and experiments. \n",
    "\n",
    "For this tutorial, we will create an auto-scaling [Azure Machine Learning Compute](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.compute.amlcompute?view=azure-ml-py) cluster, which is a managed-compute infrastructure that allows the user to easily create a single or multi-node compute. To create the cluster, we need to specify the following parameters:\n",
    "\n",
    "- `vm_size`: The is the type of GPUs that we want to use in our cluster. For this tutorial, we will use **Standard_NC6s_v3 (NVIDIA V100) GPU Machines** .\n",
    "- `idle_seconds_before_scaledown`: This is the number of seconds before a node will scale down in our auto-scaling cluster. We will set this to **1800** seconds. \n",
    "- `min_nodes`: This is the minimum numbers of nodes that the cluster will have. To avoid paying for compute while they are not being used, we will set this to **0** nodes.\n",
    "- `max_modes`: This is the maximum number of nodes that the cluster will scale up to. Will will set this to **2** nodes.\n",
    "\n",
    "**Creation of the cluster takes approximately 5 minutes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "cluster_name = 'v100cluster'\n",
    "compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6s_v3', \n",
    "                                                       idle_seconds_before_scaledown=1800,\n",
    "                                                       min_nodes=0, \n",
    "                                                       max_nodes=2)\n",
    "\n",
    "compute_target = ComputeTarget.create(workspace, cluster_name, compute_config)\n",
    "compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure our compute target was created successfully, we can check it's status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_target.get_status().serialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the compute target has already been created, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compute_target = workspace.compute_targets['v100cluster']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Datastore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [Datastore](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.datastore.datastore?view=azure-ml-py) is used to store connection information to a central data storage. This allows you to access your storage without having to hard code this (potentially confidential) information into your scripts. \n",
    "\n",
    "In this tutorial, the data was been previously prepped and uploaded into a central [Blob Storage](https://azure.microsoft.com/en-us/services/storage/blobs/) container. We will register this container into our workspace as a datastore using a [shared access signature (SAS) token](https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "datastore_name = 'tfworld'\n",
    "container_name = 'azureml-blobstore-7c6bdd88-21fa-453a-9c80-16998f02935f'\n",
    "account_name = 'tfworld6818510241'\n",
    "account_key = 'ub7Rw5HQ+lOQR5Ji8wFSzD/m3nov3hM1T6+y6jFxkiXjYi5bl3HJ7lTt8kI5nx+EiHZpp2QEpcpUj3FwhWkSXA=='\n",
    "# sas_token = '?sv=2019-02-02&ss=bfqt&srt=sco&sp=r&se=2019-11-06T02:30:14Z&st=2019-10-22T17:30:14Z&spr=https&sig=hT1kTCp3yrvQ1JbNTRLd06j4J%2FlZYSat0YKneOvigJM%3D'\n",
    "\n",
    "datastore = Datastore.register_azure_blob_container(workspace=workspace, \n",
    "                                                    datastore_name=datastore_name, \n",
    "                                                    container_name=container_name,\n",
    "                                                    account_name=account_name, \n",
    "                                                    account_key=account_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the datastore has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datastore = workspace.datastores['tfworld']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if my data wasn't already hosted remotely?\n",
    "All workspaces also come with a blob container which is registered as a default datastore. This allows you to easily upload your own data to a remote storage location. You can access this datastore and upload files as follows:\n",
    "```\n",
    "datastore = workspace.get_default_datastore()\n",
    "ds.upload(src_dir='<LOCAL-PATH>', target_path='<REMOTE-PATH>')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Dataset\n",
    "\n",
    "A [Dataset](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.dataset.dataset?view=azure-ml-py) is a reference to data in a datastore. We can register specific folders in our datastore that contains our data files, as a Dataset. This allows you to have direct access to your data directory within a datastore.\n",
    "\n",
    "There is a folder within our datastore called **azure-service-data** that contains all our training and testing data. We will register this as a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azure_dataset = Dataset.File.from_files(path=(datastore, 'azure-service-classifier/data'))\n",
    "\n",
    "azure_dataset = azure_dataset.register(workspace=workspace,\n",
    "                                       name='Azure Services Dataset',\n",
    "                                       description='Dataset containing azure related posts on Stackoverflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If the dataset has already been registered, then you (and other users in your workspace) can directly run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azure_dataset = workspace.datasets['Azure Services Dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to keep your [training scripts](https://github.com/johnwu0604/azure-service-classifier) separated from your notebook. We have prepared a script in advance that trains a BERT model on the dataset using Tensorflow 2.0 and the open source [huggingface/transformers](https://github.com/huggingface/transformers) libary. Let's start by downloading that into a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "project_folder = './azure-service-classifier'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/johnwu0604/azure-service-classifier $project_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the *train.py* script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pycat ./azure-service-classifier/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Locally\n",
    "\n",
    "Let's try running the script locally to make sure that it works. To do so, you will need to install Tensorflow 2.0 and the transformers libary first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow-gpu as it is not installed.\u001b[0m\n",
      "Collecting tensorflow-gpu==2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
      "\u001b[K     |████████████████████████████████| 380.8MB 22kB/s s eta 0:00:01  |                                | 215kB 4.8MB/s eta 0:01:20     |█▉                              | 22.4MB 4.8MB/s eta 0:01:15     |████                            | 48.3MB 4.8MB/s eta 0:01:10     |█████████▊                      | 115.5MB 65.0MB/s eta 0:00:05     |██████████████▏                 | 168.0MB 65.0MB/s eta 0:00:04     |█████████████████▊              | 210.9MB 48.5MB/s eta 0:00:04��██▉           | 247.5MB 70.5MB/s eta 0:00:02     |█████████████████████▋          | 257.6MB 70.5MB/s eta 0:00:02�███████████▊       | 294.1MB 70.5MB/s eta 0:00:02     |█████████████████████████▌      | 304.0MB 64.5MB/s eta 0:00:02     |██████████████████████████▋     | 317.0MB 64.5MB/s eta 0:00:01     |███████████████████████████▉    | 331.7MB 64.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\n",
      "\u001b[K     |████████████████████████████████| 296kB 47.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.16.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
      "\u001b[K     |████████████████████████████████| 450kB 46.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.24.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (3.9.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.29.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/83/755bd5324777875e9dff19c2e59daec837d0378c09196634524a3d7269ac/opt_einsum-3.1.0.tar.gz (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 2.2MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<2.1.0,>=2.0.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 37.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
      "Collecting regex\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/60/d9782c56ceefa76033a00e1f84cd8c586c75e6e7fea2cd45ee8b46a386c5/regex-2019.08.19-cp36-cp36m-manylinux1_x86_64.whl (643kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 41.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers==2.0.0) (2.22.0)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
      "\u001b[K     |████████████████████████████████| 860kB 33.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 36.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers==2.0.0) (4.36.1)\n",
      "Requirement already satisfied: boto3 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from transformers==2.0.0) (1.9.239)\n",
      "Requirement already satisfied: setuptools in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.2.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: h5py in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.7.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers==2.0.0) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers==2.0.0) (1.25.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers==2.0.0) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from requests->transformers==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: click in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers==2.0.0) (7.0)\n",
      "Requirement already satisfied: joblib in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from sacremoses->transformers==2.0.0) (0.13.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers==2.0.0) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers==2.0.0) (0.2.1)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.239 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from boto3->transformers==2.0.0) (1.12.239)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.239->boto3->transformers==2.0.0) (2.8.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda/envs/azureml_py36/lib/python3.6/site-packages (from botocore<1.13.0,>=1.12.239->boto3->transformers==2.0.0) (0.14)\n",
      "Building wheels for collected packages: gast, opt-einsum, sacremoses\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7635 sha256=f6dc78294bf796e9e028b72c86ae422c4352f6b021059496d7d1386f30442dae\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for opt-einsum (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for opt-einsum: filename=opt_einsum-3.1.0-cp36-none-any.whl size=63905 sha256=8fcfe8e70f7704af78c7bdadb7cb49836226377a1d0963f6eb0a3fe044abf8b5\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/2c/b1/94/43d03e130b929aae7ba3f8d15cbd7bc0d1cb5bb38a5c721833\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=884754 sha256=ab4ab2f4ac3513a2c2c5d1a1951fecccd1236d34389d2915573c524736188188\n",
      "  Stored in directory: /home/azureuser/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
      "Successfully built gast opt-einsum sacremoses\n",
      "Installing collected packages: tensorflow-estimator, gast, opt-einsum, tensorboard, tensorflow-gpu, regex, sacremoses, sentencepiece, transformers\n",
      "  Found existing installation: tensorflow-estimator 1.14.0\n",
      "    Uninstalling tensorflow-estimator-1.14.0:\n",
      "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
      "  Found existing installation: gast 0.3.2\n",
      "    Uninstalling gast-0.3.2:\n",
      "      Successfully uninstalled gast-0.3.2\n",
      "  Found existing installation: tensorboard 1.14.0\n",
      "    Uninstalling tensorboard-1.14.0:\n",
      "      Successfully uninstalled tensorboard-1.14.0\n",
      "Successfully installed gast-0.2.2 opt-einsum-3.1.0 regex-2019.8.19 sacremoses-0.0.35 sentencepiece-0.1.83 tensorboard-2.0.0 tensorflow-estimator-2.0.1 tensorflow-gpu-2.0.0 transformers-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow-gpu --yes\n",
    "!pip install tensorflow-gpu==2.0.0 transformers==2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run our script and train for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: Syntax error: \"(\" unexpected\r\n"
     ]
    }
   ],
   "source": [
    "!python ./azure-service-classifier/train.py --data_dir $azure_dataset.as_named_input('azureservicedata').as_mount() --max_seq_length 128 --batch_size 32 --learning_rate 3e-5 --steps_per_epoch 10 --num_epochs 1 --export_dir ./outputs/exports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Experiment\n",
    "\n",
    "Now that we have our compute target, dataset, and training script ready, it is time to create an [experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment.experiment?view=azure-ml-py). An experiment is a grouping of many runs from a specified script. All runs in this tutorial will be performed under the same experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'azure-service-classifier' \n",
    "experiment = Experiment(workspace, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create TensorFlow Estimator\n",
    "\n",
    "The Azure Machine Learning Python SDK Estimator classes allow you to easily construct run configurations for your experiments. They allow you too define parameters such as the training script to run, the compute target to run it on, framework versions, additional package requirements, etc. \n",
    "\n",
    "You can also use a generic [Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.estimator.estimator?view=azure-ml-py) to submit training scripts that use any learning framework you choose.\n",
    "\n",
    "For popular libaries like PyTorch and Tensorflow you can use their framework specific estimators. We will use the [TensorFlow Estimator](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.dnn.tensorflow?view=azure-ml-py) for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "estimator1 = TensorFlow(source_directory=project_folder,\n",
    "                        entry_script='train.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/exports'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.22'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick description for each of the parameters we have just defined:\n",
    "\n",
    "- `source_directory`: This specifies the root directory of our source code. \n",
    "- `entry_script`: This specifies the training script to run. It should be relative to the source_directory.\n",
    "- `compute_target`: This specifies to compute target to run the job on. We will use the one created earlier.\n",
    "- `script_params`: This specifies the input parameters to the training script. Please note:\n",
    "\n",
    "    1) *azure_dataset.as_named_input('azureservicedata').as_mount()* mounts the dataset to the remote compute and provides the path to the dataset on our datastore. \n",
    "    \n",
    "    2) All outputs from the training script must be outputted to an './outputs' directory as this is the only directory that will be saved to the run. \n",
    "    \n",
    "    \n",
    "- `framework_version`: This specifies the version of TensorFlow to use. Use Tensorflow.get_supported_verions() to see all supported versions.\n",
    "- `use_gpu`: This will use the GPU on the compute target for training if set to True.\n",
    "- `pip_packages`: This allows you to define any additional libraries to install before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Submit First Run \n",
    "\n",
    "We can now train our model by submitting the estimator object as a [run](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.run.run?view=azure-ml-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run1 = experiment.submit(estimator1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the current status of the run and stream the logs from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cancel a run at anytime which will stop the run and scale down the nodes in the compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run1.cancel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Add Metrics Logging\n",
    "\n",
    "So we were able to clone a Tensorflow 2.0 project and run it without any changes. However, with larger scale projects we would want to log some metrics in order to make it easier to monitor the performance of our model. \n",
    "\n",
    "We can do this by adding a few lines of code into our training script:\n",
    "\n",
    "```python\n",
    "# 1) Import SDK Run object\n",
    "from azureml.core.run import Run\n",
    "\n",
    "# 2) Get current service context\n",
    "run = Run.get_context()\n",
    "\n",
    "# 3) Log the metrics that we want\n",
    "run.log('val_accuracy', float(logs.get('val_accuracy')))\n",
    "run.log('accuracy', float(logs.get('accuracy')))\n",
    "```\n",
    "We've created a *train_logging.py* script that includes logging metrics as shown above. Let's see what the updated script looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pycat ./azure-service-classifier/train_logging.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did before. \n",
    "\n",
    "*Since our cluster can scale automatically to two nodes, we can run this job simultaneously with the previous one.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator2 = TensorFlow(source_directory=project_folder,\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target, \n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/exports'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.22'])\n",
    "\n",
    "run2 = experiment.submit(estimator2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we view the current details of the run, you will notice that the metrics will be logged into graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(run2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we wait for our two runs to complete, let's go over how a Run is executed in Azure Machine Learning.\n",
    "\n",
    "![](./images/aml-run.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Distributed Training Across Multiple GPUs\n",
    "\n",
    "Distributed training allows us to train across multiple nodes if your cluster allows it. Azure Machine Learning service helps manage the infrastructure for training distributed jobs. All we have to do is add the following parameters to our estimator object in order to enable this:\n",
    "\n",
    "- `node_count`: The number of nodes to run this job across. Our cluster has a maximum node limit of 2, so we will set this to 2.\n",
    "- `process_count_per_node`: The number of processes to enable per node. We will set this to 1.\n",
    "- `distributed_training`: The backend to use for our distributed job. We will be using an MPI (Message Passing Interface) backend which is a standardized design for message passing.\n",
    "\n",
    "We use [horovod](https://github.com/horovod/horovod), which is a library that allows us to easily modifying our existing training script to be run across multiple nodes. The distributed training script is saved as *train_horovod.py*. Let's see what the updated script looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pycat ./azure-service-classifier/train_horovod.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this run in the same way that we did with the others, but with the additional parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import MpiConfiguration\n",
    "\n",
    "estimator3 = TensorFlow(source_directory=project_folder,\n",
    "                        entry_script='train_horovod.py',compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--learning_rate': 3e-5,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--num_epochs': 3,\n",
    "                              '--export_dir':'./outputs/exports'\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        node_count=2,\n",
    "                        process_count_per_node=1,\n",
    "                        distributed_training=MpiConfiguration(),\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.22'])\n",
    "\n",
    "run3 = experiment.submit(estimator3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, we can view the current details of the run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(run3).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: In theory, a distributed job should run faster than a non-distributed one but **only if the amount speedup gained from parallelism is greater than the bottleneck introduced in the MPI**. Therefore on shorter training jobs like this one, a distributed job wouldn't always be faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Tune Hyperparameters Using Hyperdrive\n",
    "\n",
    "So far we have been putting in default hyperparameter values, but in practice we would need tune these values to optimize the performance. Azure Machine Learning service provides many methods for tuning hyperparameters using different strategies.\n",
    "\n",
    "The first step is to choose the parameter space that we want to search. We have a few choices to make here :\n",
    "\n",
    "- **Parameter Sampling Method**: This is how we select the combinations of parameters to sample. Azure Machine Learning service offers [RandomParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.randomparametersampling?view=azure-ml-py), [GridParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.gridparametersampling?view=azure-ml-py), and [BayesianParameterSampling](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.bayesianparametersampling?view=azure-ml-py). We will use the `GridParameterSampling` method.\n",
    "- **Parameters To Search**: We will be searching for optimal combinations of `learning_rate` and `num_epochs`.\n",
    "- **Parameter Expressions**: This defines the [functions that can be used to describe a hyperparameter search space](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.parameter_expressions?view=azure-ml-py), which can be discrete or continuous. We will be using a `discrete set of choices`.\n",
    "\n",
    "The following code allows us to define these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import GridParameterSampling\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice\n",
    "\n",
    "\n",
    "param_sampling = GridParameterSampling( {\n",
    "        '--learning_rate': choice(3e-5, 3e-4),\n",
    "        '--num_epochs': choice(3, 4)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to a define how we want to measure our performance. We do so by specifying two classes:\n",
    "\n",
    "- **[PrimaryMetricGoal](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.primarymetricgoal?view=azure-ml-py)**: We want to `MAXIMIZE` the `val_accuracy` that is logged in our training script.\n",
    "- **[BanditPolicy](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.banditpolicy?view=azure-ml-py)**: A policy for early termination so that jobs which don't show promising results will stop automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import BanditPolicy\n",
    "from azureml.train.hyperdrive import PrimaryMetricGoal\n",
    "\n",
    "primary_metric_name='val_accuracy'\n",
    "primary_metric_goal=PrimaryMetricGoal.MAXIMIZE\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval=1, delay_evaluation=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define an estimator as usual, but this time without the script parameters that we are planning to search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator4 = TensorFlow(source_directory=project_folder,\n",
    "                        entry_script='train_logging.py',\n",
    "                        compute_target=compute_target,\n",
    "                        script_params = {\n",
    "                              '--data_dir': azure_dataset.as_named_input('azureservicedata').as_mount(),\n",
    "                              '--max_seq_length': 128,\n",
    "                              '--batch_size': 32,\n",
    "                              '--steps_per_epoch': 150,\n",
    "                              '--export_dir':'./outputs/exports',\n",
    "                        },\n",
    "                        framework_version='2.0',\n",
    "                        use_gpu=True,\n",
    "                        pip_packages=['transformers==2.0.0', 'azureml-dataprep[fuse,pandas]==1.1.22'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add all our parameters in a [HyperDriveConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py) class and submit it as a run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive import HyperDriveConfig\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(estimator=estimator4,\n",
    "                                         hyperparameter_sampling=param_sampling, \n",
    "                                         policy=early_termination_policy,\n",
    "                                         primary_metric_name=primary_metric_name, \n",
    "                                         primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                         max_total_runs=10,\n",
    "                                         max_concurrent_runs=2)\n",
    "\n",
    "run4 = experiment.submit(hyperdrive_run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we view the details of our run this time, we will see information and metrics for every run in our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(run4).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can retrieve the best run based on our defined metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run = run4.get_best_run_by_primary_metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Model\n",
    "\n",
    "A registered [model](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model(class)?view=azure-ml-py) is a reference to the directory or file that make up your model. After registering a model, you and other people in your workspace can easily gain access to and deploy your model without having to run the training script again. \n",
    "\n",
    "We need to define the following parameters to register a model:\n",
    "\n",
    "- `model_name`: The name for your model. If the model name already exists in the workspace, it will create a new version for the model.\n",
    "- `model_path`: The path to where the model is stored. In our case, this was the *export_dir* defined in our estimators.\n",
    "- `description`: A description for the model.\n",
    "\n",
    "Let's register the best run from our hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name='azure-service-classifier', model_path='./outputs/exports', description='BERT model for classifying azure services on stackoverflow posts.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the [next tutorial](), we will perform inferencing on this model and deploy it to a web service."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
