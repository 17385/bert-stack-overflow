{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability With Tensorflow On Azure Machine Learning Service\n",
    "\n",
    "## Overview of Tutorial\n",
    "This notebook is Part 4 (Explaining Your Model Using Interpretability) of a four part workshop that demonstrates an end-to-end workflow for using Tensorflow on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Preparing Data and Model Training](https://github.com/microsoft/bert-stack-overflow/blob/master/1-Training/AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/microsoft/bert-stack-overflow/blob/master/2-Inferencing/AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/microsoft/bert-stack-overflow/tree/master/3-ML-Ops)\n",
    "- Part 4: [Explaining Your Model Interpretability](https://github.com/microsoft/bert-stack-overflow/blob/master/4-Interpretibility/IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "**In this specific tutorial, we will cover the following topics:**\n",
    "\n",
    "- TODO\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Machine Learning Interpretability?\n",
    "Interpretability is the ability to explain why your model made the predictions it did. The Azure Machine Learning service offers various interpretability features to help accomplish this task. These features include:\n",
    "\n",
    "- Feature importance values for both raw and engineered features.\n",
    "- Interpretability on real-world datasets at scale, during training and inference.\n",
    "- Interactive visualizations to aid you in the discovery of patterns in data and explanations at training time.\n",
    "\n",
    "By accurately interpretabiliting your model, it allows you to:\n",
    "\n",
    "- Use the insights for debugging your model.\n",
    "- Validate model behavior matches their objectives.\n",
    "- Check for for bias in the model.\n",
    "- Build trust in your customers and stakeholders.\n",
    "\n",
    "![](./images/interpretability-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Azure Machine Learning Python SDK\n",
    "\n",
    "If you are running this on a Notebook VM, the Azure Machine Learning Python SDK is installed by default. If you are running this locally, you can follow these [instructions](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py) to install it using pip.\n",
    "\n",
    "This tutorial series requires version 1.0.69 or higher. We can import the Python SDK to ensure it has been properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure Machine Learning Python SDK version: 1.0.69\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tensorflow 1.14\n",
    "\n",
    "We will be using an older version (1.14) for this particular tutorial in the series as Tensorflow 2.0 is not yet supported for Interpretibility on Azure Machine Learning service. If are currently running Tensorflow 2.0, run the code below to downgrade the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall tensorflow-gpu keras --yes\n",
    "%pip install tensorflow-gpu==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have the right verison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "Just like in the previous tutorials, we will need to connect to a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py).\n",
    "\n",
    "The following code will allow you to create a workspace if you don't already have one created. You must have an Azure subscription to create a workspace:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2')\n",
    "```\n",
    "\n",
    "**If you are running this on a Notebook VM, you can import the existing workspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: tf-world\n",
      "Azure region: eastus\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: tf-world\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that the above commands reads a config.json file that exists by default within the Notebook VM. If you are running this locally or want to use a different workspace, you must add a config file to your project directory. The config file should have the following schema:\n",
    "\n",
    "```\n",
    "    {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION-ID>\",\n",
    "        \"resource_group\": \"<RESOURCE-GROUP>\",\n",
    "        \"workspace_name\": \"<WORKSPACE-NAME>\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "For this tutorial, we will be using the *tf.keras module* to train a basic feed forward neural network on the IBM Employee Attrition Dataset. \n",
    "\n",
    "**We will start by writing the training script to train our model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING - From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1176 samples, validate on 294 samples\n",
      "Epoch 1/20\n",
      "1176/1176 [==============================] - 0s 175us/sample - loss: 0.5150 - acc: 0.8308 - val_loss: 0.4791 - val_acc: 0.8401\n",
      "Epoch 2/20\n",
      "1176/1176 [==============================] - 0s 19us/sample - loss: 0.4637 - acc: 0.8376 - val_loss: 0.4502 - val_acc: 0.8401\n",
      "Epoch 3/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.4375 - acc: 0.8376 - val_loss: 0.4307 - val_acc: 0.8401\n",
      "Epoch 4/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.4187 - acc: 0.8384 - val_loss: 0.4157 - val_acc: 0.8401\n",
      "Epoch 5/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.4052 - acc: 0.8384 - val_loss: 0.4067 - val_acc: 0.8401\n",
      "Epoch 6/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3958 - acc: 0.8384 - val_loss: 0.3992 - val_acc: 0.8401\n",
      "Epoch 7/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3879 - acc: 0.8384 - val_loss: 0.3936 - val_acc: 0.8401\n",
      "Epoch 8/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3800 - acc: 0.8384 - val_loss: 0.3875 - val_acc: 0.8401\n",
      "Epoch 9/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3725 - acc: 0.8384 - val_loss: 0.3834 - val_acc: 0.8401\n",
      "Epoch 10/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3660 - acc: 0.8384 - val_loss: 0.3788 - val_acc: 0.8401\n",
      "Epoch 11/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3594 - acc: 0.8401 - val_loss: 0.3753 - val_acc: 0.8401\n",
      "Epoch 12/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3520 - acc: 0.8401 - val_loss: 0.3700 - val_acc: 0.8401\n",
      "Epoch 13/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3450 - acc: 0.8418 - val_loss: 0.3686 - val_acc: 0.8401\n",
      "Epoch 14/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3386 - acc: 0.8435 - val_loss: 0.3668 - val_acc: 0.8401\n",
      "Epoch 15/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3323 - acc: 0.8461 - val_loss: 0.3633 - val_acc: 0.8469\n",
      "Epoch 16/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3262 - acc: 0.8512 - val_loss: 0.3598 - val_acc: 0.8537\n",
      "Epoch 17/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3202 - acc: 0.8588 - val_loss: 0.3589 - val_acc: 0.8537\n",
      "Epoch 18/20\n",
      "1176/1176 [==============================] - 0s 17us/sample - loss: 0.3147 - acc: 0.8597 - val_loss: 0.3563 - val_acc: 0.8639\n",
      "Epoch 19/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3094 - acc: 0.8639 - val_loss: 0.3541 - val_acc: 0.8571\n",
      "Epoch 20/20\n",
      "1176/1176 [==============================] - 0s 16us/sample - loss: 0.3044 - acc: 0.8724 - val_loss: 0.3518 - val_acc: 0.8639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb2a5761550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(data):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "    data = data.drop(['EmployeeCount'], axis=1)\n",
    "    \n",
    "    # Dropping Employee Number since it is merely an identifier\n",
    "    data = data.drop(['EmployeeNumber'], axis=1)\n",
    "    data = data.drop(['Over18'], axis=1)\n",
    "\n",
    "    # Since all values are 80\n",
    "    data = data.drop(['StandardHours'], axis=1)\n",
    "\n",
    "    # Converting target variables from string to numerical values\n",
    "    target_map = {'Yes': 1, 'No': 0}\n",
    "    data[\"Attrition_numerical\"] = data[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "    target = data[\"Attrition_numerical\"]\n",
    "\n",
    "    data.drop(['Attrition_numerical', 'Attrition'], axis=1, inplace=True)\n",
    "    \n",
    "    # Creating dummy columns for each categorical feature\n",
    "    categorical = []\n",
    "    for col, value in data.iteritems():\n",
    "        if value.dtype == 'object':\n",
    "            categorical.append(col)\n",
    "\n",
    "    # Store the numerical columns in a list numerical\n",
    "    numerical = data.columns.difference(categorical)   \n",
    "\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "    \n",
    "    pipeline = make_pipeline(preprocess)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                        target, \n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=0,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, pipeline, preprocess\n",
    "    \n",
    "# Load and preprocess data\n",
    "attrition_data = pd.read_csv('./data/data.csv')\n",
    "x_train, x_test, y_train, y_test, pipeline, preprocess = preprocess_data(attrition_data)\n",
    "\n",
    "# Transform data\n",
    "x_train_t = pipeline.fit_transform(x_train)\n",
    "x_test_t = pipeline.transform(x_test)\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu', input_shape=(x_train_t.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train_t, y_train, epochs=20, verbose=1, batch_size=128, validation_data=(x_test_t, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model Locally\n",
    "\n",
    "We will start by explaining the trained model locally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Instantiate the explainer object using trained model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/shap/explainers/deep/deep_tf.py:118: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from interpret.ext.greybox import DeepExplainer\n",
    "\n",
    "explainer = DeepExplainer(model,\n",
    "                          x_train,\n",
    "                          features=x_train.columns,\n",
    "                          classes=[\"STAYING\", \"LEAVING\"], \n",
    "                          transformations = preprocess,\n",
    "                          model_task=\"classification\",\n",
    "                          is_classifier=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate local explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local importance values: [[[0.02745935619113111, 0.02388463189410146, 0.020942007144913077, 0.02072276084300257, 0.01591142771356663, 0.01487850179481488, 0.012745772539601552, 0.009650989505462348, 0.00963469324633479, 0.008773050403817054, 0.006977637853967165, 0.005644893143529896, 0.005188455890287827, 0.0037072910350359166, 0.003232530595886378, 0.001697204236677791, 0.0016097691412437006, 0.0014667453432869921, 0.0, 0.0, 0.0, -0.0004357534955488518, -0.0017016542948617964, -0.0024226699168295633, -0.005660104808416575, -0.009158367349154066, -0.01248942909341209, -0.012904132188475203, -0.018412580084532233, -0.018687076805216047]], [[0.018687076805216047, 0.018412580084532233, 0.012904132188475203, 0.01248942909341209, 0.009158367349154066, 0.005660104808416575, 0.0024226699168295633, 0.0017016542948617964, 0.0004357534955488518, 0.0, 0.0, 0.0, -0.0014667453432869921, -0.0016097691412437006, -0.001697204236677791, -0.003232530595886378, -0.0037072910350359166, -0.005188455890287827, -0.005644893143529896, -0.006977637853967165, -0.008773050403817054, -0.00963469324633479, -0.009650989505462348, -0.012745772539601552, -0.01487850179481488, -0.01591142771356663, -0.02072276084300257, -0.020942007144913077, -0.02388463189410146, -0.02745935619113111]]]\n",
      "local importance names: [[['TotalWorkingYears', 'JobLevel', 'EducationField', 'YearsSinceLastPromotion', 'WorkLifeBalance', 'DailyRate', 'DistanceFromHome', 'JobRole', 'MaritalStatus', 'Age', 'NumCompaniesWorked', 'MonthlyIncome', 'StockOptionLevel', 'PercentSalaryHike', 'YearsAtCompany', 'JobSatisfaction', 'RelationshipSatisfaction', 'MonthlyRate', 'BusinessTravel', 'OverTime', 'Department', 'Gender', 'Education', 'PerformanceRating', 'HourlyRate', 'YearsInCurrentRole', 'JobInvolvement', 'TrainingTimesLastYear', 'YearsWithCurrManager', 'EnvironmentSatisfaction']], [['EnvironmentSatisfaction', 'YearsWithCurrManager', 'TrainingTimesLastYear', 'JobInvolvement', 'YearsInCurrentRole', 'HourlyRate', 'PerformanceRating', 'Education', 'Gender', 'BusinessTravel', 'Department', 'OverTime', 'MonthlyRate', 'RelationshipSatisfaction', 'JobSatisfaction', 'YearsAtCompany', 'PercentSalaryHike', 'StockOptionLevel', 'MonthlyIncome', 'NumCompaniesWorked', 'Age', 'MaritalStatus', 'JobRole', 'DistanceFromHome', 'DailyRate', 'WorkLifeBalance', 'YearsSinceLastPromotion', 'EducationField', 'JobLevel', 'TotalWorkingYears']]]\n"
     ]
    }
   ],
   "source": [
    "# You can pass a specific data point or a group of data points to the explain_local function\n",
    "# E.g., Explain the first data point in the test set\n",
    "instance_num = 1\n",
    "local_explanation = explainer.explain_local(x_test[:instance_num])\n",
    "\n",
    "sorted_local_importance_values = local_explanation.get_ranked_local_values()\n",
    "sorted_local_importance_names = local_explanation.get_ranked_local_names()\n",
    "\n",
    "print('local importance values: {}'.format(sorted_local_importance_values))\n",
    "print('local importance names: {}'.format(sorted_local_importance_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate global explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global importance rank: {'NumCompaniesWorked': 0.03417471731378875, 'EducationField': 0.03277963455957586, 'OverTime': 0.03186986137032637, 'StockOptionLevel': 0.027491102002814863, 'DistanceFromHome': 0.02726175194254244, 'DailyRate': 0.026943844012963694, 'Department': 0.02673539651506227, 'YearsSinceLastPromotion': 0.0242831720212505, 'EnvironmentSatisfaction': 0.023014777976693532, 'JobRole': 0.021821173867686894, 'TotalWorkingYears': 0.021542080085869175, 'TrainingTimesLastYear': 0.020734608450857426, 'YearsWithCurrManager': 0.020032447284073274, 'RelationshipSatisfaction': 0.018775204327377625, 'YearsInCurrentRole': 0.01746664967347312, 'JobInvolvement': 0.014602441754769383, 'MaritalStatus': 0.014237106674383018, 'WorkLifeBalance': 0.014229805121233768, 'JobSatisfaction': 0.01347278631249912, 'JobLevel': 0.012899359471362863, 'Age': 0.01128978592129426, 'HourlyRate': 0.009253448266305697, 'PercentSalaryHike': 0.00885626675733648, 'BusinessTravel': 0.008690116201352011, 'Gender': 0.007315523818717685, 'PerformanceRating': 0.006569386481453725, 'Education': 0.006429576965858625, 'MonthlyIncome': 0.006015271597339995, 'MonthlyRate': 0.0059935155732225185, 'YearsAtCompany': 0.0054440447010206975}\n",
      "ranked per class feature names: [['NumCompaniesWorked', 'EducationField', 'OverTime', 'StockOptionLevel', 'DistanceFromHome', 'DailyRate', 'Department', 'YearsSinceLastPromotion', 'EnvironmentSatisfaction', 'JobRole', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsWithCurrManager', 'RelationshipSatisfaction', 'YearsInCurrentRole', 'JobInvolvement', 'MaritalStatus', 'WorkLifeBalance', 'JobSatisfaction', 'JobLevel', 'Age', 'HourlyRate', 'PercentSalaryHike', 'BusinessTravel', 'Gender', 'PerformanceRating', 'Education', 'MonthlyIncome', 'MonthlyRate', 'YearsAtCompany'], ['NumCompaniesWorked', 'EducationField', 'OverTime', 'StockOptionLevel', 'DistanceFromHome', 'DailyRate', 'Department', 'YearsSinceLastPromotion', 'EnvironmentSatisfaction', 'JobRole', 'TotalWorkingYears', 'TrainingTimesLastYear', 'YearsWithCurrManager', 'RelationshipSatisfaction', 'YearsInCurrentRole', 'JobInvolvement', 'MaritalStatus', 'WorkLifeBalance', 'JobSatisfaction', 'JobLevel', 'Age', 'HourlyRate', 'PercentSalaryHike', 'BusinessTravel', 'Gender', 'PerformanceRating', 'Education', 'MonthlyIncome', 'MonthlyRate', 'YearsAtCompany']]\n",
      "ranked per class feature values: [[0.03417471731378875, 0.03277963455957586, 0.03186986137032637, 0.027491102002814863, 0.02726175194254244, 0.026943844012963694, 0.02673539651506227, 0.0242831720212505, 0.023014777976693532, 0.021821173867686894, 0.021542080085869175, 0.020734608450857426, 0.020032447284073274, 0.018775204327377625, 0.01746664967347312, 0.014602441754769383, 0.014237106674383018, 0.014229805121233768, 0.01347278631249912, 0.012899359471362863, 0.01128978592129426, 0.009253448266305697, 0.00885626675733648, 0.008690116201352011, 0.007315523818717685, 0.006569386481453725, 0.006429576965858625, 0.006015271597339995, 0.0059935155732225185, 0.0054440447010206975], [0.03417471731378875, 0.03277963455957586, 0.03186986137032637, 0.027491102002814863, 0.02726175194254244, 0.026943844012963694, 0.02673539651506227, 0.0242831720212505, 0.023014777976693532, 0.021821173867686894, 0.021542080085869175, 0.020734608450857426, 0.020032447284073274, 0.018775204327377625, 0.01746664967347312, 0.014602441754769383, 0.014237106674383018, 0.014229805121233768, 0.01347278631249912, 0.012899359471362863, 0.01128978592129426, 0.009253448266305697, 0.00885626675733648, 0.008690116201352011, 0.007315523818717685, 0.006569386481453725, 0.006429576965858625, 0.006015271597339995, 0.0059935155732225185, 0.0054440447010206975]]\n"
     ]
    }
   ],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# x_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(x_test)\n",
    "\n",
    "# Print out a dictionary that holds the sorted feature importance names and values\n",
    "print('global importance rank: {}'.format(global_explanation.get_feature_importance_dict()))\n",
    "\n",
    "# Per class feature names\n",
    "print('ranked per class feature names: {}'.format(global_explanation.get_ranked_per_class_names()))\n",
    "\n",
    "# Per class feature importance values\n",
    "print('ranked per class feature values: {}'.format(global_explanation.get_ranked_per_class_values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize our explanations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a3c428474304950b4d1e2427a354e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ExplanationWidget(value={'predictedY': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<interpret_community.widget.ExplanationDashboard.ExplanationDashboard at 0x7fb1fd1decc0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from interpret_community.widget import ExplanationDashboard\n",
    "from interpret_community.common.model_wrapper import wrap_model\n",
    "from interpret_community.dataset.dataset_wrapper import DatasetWrapper\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "wrapped_model, ml_domain = wrap_model(model, DatasetWrapper(x_test_t), \"classification\")\n",
    "wrapped_model.fit = model.fit\n",
    "dashboard_pipeline = Pipeline(steps=[('preprocess', preprocess), ('network', wrapped_model)])\n",
    "ExplanationDashboard(global_explanation, dashboard_pipeline, datasetX=x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model On Azure Machine Learning Service\n",
    "\n",
    "Now let's train our model on Azure Machine Learning service and explain remotely.\n",
    "\n",
    "**Instead of running our script in the notebook, let's start by writing the script to a train.py file.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(data):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "    data = data.drop(['EmployeeCount'], axis=1)\n",
    "    \n",
    "    # Dropping Employee Number since it is merely an identifier\n",
    "    data = data.drop(['EmployeeNumber'], axis=1)\n",
    "    data = data.drop(['Over18'], axis=1)\n",
    "\n",
    "    # Since all values are 80\n",
    "    data = data.drop(['StandardHours'], axis=1)\n",
    "\n",
    "    # Converting target variables from string to numerical values\n",
    "    target_map = {'Yes': 1, 'No': 0}\n",
    "    data[\"Attrition_numerical\"] = data[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "    target = data[\"Attrition_numerical\"]\n",
    "\n",
    "    data.drop(['Attrition_numerical', 'Attrition'], axis=1, inplace=True)\n",
    "    \n",
    "    # Creating dummy columns for each categorical feature\n",
    "    categorical = []\n",
    "    for col, value in data.iteritems():\n",
    "        if value.dtype == 'object':\n",
    "            categorical.append(col)\n",
    "\n",
    "    # Store the numerical columns in a list numerical\n",
    "    numerical = data.columns.difference(categorical)   \n",
    "\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "    \n",
    "    pipeline = make_pipeline(preprocess)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                        target, \n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=0,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, pipeline, preprocess\n",
    "    \n",
    "# Load and preprocess data\n",
    "attrition_data = pd.read_csv('./data/data.csv')\n",
    "x_train, x_test, y_train, y_test, pipeline, preprocess = preprocess_data(attrition_data)\n",
    "\n",
    "# Transform data\n",
    "x_train_t = pipeline.fit_transform(x_train)\n",
    "x_test_t = pipeline.transform(x_test)\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu', input_shape=(x_train_t.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train_t, y_train, epochs=20, verbose=1, batch_size=128, validation_data=(x_test_t, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save('./outputs/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now submit the script to be run on the cluster that was created in tutorial 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.dnn import TensorFlow\n",
    "\n",
    "compute_target = workspace.compute_targets['v100cluster']\n",
    "\n",
    "estimator = TensorFlow(source_directory='.',\n",
    "                        entry_script='train.py',\n",
    "                        compute_target=compute_target,\n",
    "                        framework_version='1.13',\n",
    "                        use_gpu=True)\n",
    "\n",
    "run = experiment.submit(estimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monitor the run as usual**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(run1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Register the trained model which we will use to explain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run.register_model(model_name='ibm-attrition-classifier', \n",
    "                                model_path='./outputs/model.h5',\n",
    "                                description='IBM Employee Attrition data classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Explain on AML Compute**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain A Deployed Model\n",
    "\n",
    "Now let's deploy our model and explain during runtime.\n",
    "\n",
    "**TODO**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "Learn about other use cases of the explain package on a:\n",
    "       \n",
    "1. [Training time: regression problem](./explain-regression-local.ipynb)\n",
    "1. [Training time: binary classification problem](./explain-binary-classification-local.ipynb)\n",
    "1. [Training time: multiclass classification problem](./explain-multiclass-classification-local.ipynb)\n",
    "1. [Explain models with advanced feature transformations](./advanced-feature-transformations-explain-local.ipynb)\n",
    "1. [Save model explanations via Azure Machine Learning Run History](../azure-integration/run-history/save-retrieve-explanations-run-history.ipynb)\n",
    "1. [Run explainers remotely on Azure Machine Learning Compute (AMLCompute)](../azure-integration/remote-explanation/explain-model-on-amlcompute.ipynb)\n",
    "1. Inferencing time: deploy a classification model and explainer:\n",
    "    1. [Deploy a locally-trained model and explainer](../azure-integration/scoring-time/train-explain-model-locally-and-deploy.ipynb)\n",
    "    1. [Deploy a remotely-trained model and explainer](../azure-integration/scoring-time/train-explain-model-on-amlcompute-and-deploy.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
