{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretability With Tensorflow On Azure Machine Learning Service\n",
    "\n",
    "## Overview of Tutorial\n",
    "This notebook is Part 4 (Explaining Your Model Using Interpretability) of a four part workshop that demonstrates an end-to-end workflow for using Tensorflow on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Preparing Data and Model Training](https://github.com/microsoft/bert-stack-overflow/blob/master/1-Training/AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](https://github.com/microsoft/bert-stack-overflow/blob/master/2-Inferencing/AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps](https://github.com/microsoft/bert-stack-overflow/tree/master/3-ML-Ops)\n",
    "- Part 4: [Explaining Your Model Interpretability](https://github.com/microsoft/bert-stack-overflow/blob/master/4-Interpretibility/IBMEmployeeAttritionClassifier_Interpretability.ipynb)\n",
    "\n",
    "**In this specific tutorial, we will cover the following topics:**\n",
    "\n",
    "- TODO\n",
    "- TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Azure Machine Learning Service?\n",
    "Azure Machine Learning service is a cloud service that you can use to develop and deploy machine learning models. Using Azure Machine Learning service, you can track your models as you build, train, deploy, and manage them, all at the broad scale that the cloud provides.\n",
    "![](./images/aml-overview.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is Machine Learning Interpretability?\n",
    "Interpretability is the ability to explain why your model made the predictions it did. The Azure Machine Learning service offers various interpretability features to help accomplish this task. These features include:\n",
    "\n",
    "- Feature importance values for both raw and engineered features.\n",
    "- Interpretability on real-world datasets at scale, during training and inference.\n",
    "- Interactive visualizations to aid you in the discovery of patterns in data and explanations at training time.\n",
    "\n",
    "By accurately interpretabiliting your model, it allows you to:\n",
    "\n",
    "- Use the insights for debugging your model.\n",
    "- Validate model behavior matches their objectives.\n",
    "- Check for for bias in the model.\n",
    "- Build trust in your customers and stakeholders.\n",
    "\n",
    "![](./images/interpretability-architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Azure Machine Learning Python SDK\n",
    "\n",
    "If you are running this on a Notebook VM, the Azure Machine Learning Python SDK is installed by default. If you are running this locally, you can follow these [instructions](https://docs.microsoft.com/en-us/python/api/overview/azure/ml/install?view=azure-ml-py) to install it using pip.\n",
    "\n",
    "This tutorial series requires version 1.0.69 or higher. We can import the Python SDK to ensure it has been properly installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tensorflow 1.14\n",
    "\n",
    "We will be using an older version (1.14) for this particular tutorial in the series as Tensorflow 2.0 is not yet supported for Interpretibility on Azure Machine Learning service. If are currently running Tensorflow 2.0, run the code below to downgrade the version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall tensorflow-gpu\n",
    "%pip install tensorflow-gpu==1.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure we have the right verison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.version.VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect To Workspace\n",
    "\n",
    "Just like in the previous tutorials, we will need to connect to a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class)?view=azure-ml-py).\n",
    "\n",
    "The following code will allow you to create a workspace if you don't already have one created. You must have an Azure subscription to create a workspace:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace\n",
    "ws = Workspace.create(name='myworkspace',\n",
    "                      subscription_id='<azure-subscription-id>',\n",
    "                      resource_group='myresourcegroup',\n",
    "                      create_resource_group=True,\n",
    "                      location='eastus2')\n",
    "```\n",
    "\n",
    "**If you are running this on a Notebook VM, you can import the existing workspace.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** that the above commands reads a config.json file that exists by default within the Notebook VM. If you are running this locally or want to use a different workspace, you must add a config file to your project directory. The config file should have the following schema:\n",
    "\n",
    "```\n",
    "    {\n",
    "        \"subscription_id\": \"<SUBSCRIPTION-ID>\",\n",
    "        \"resource_group\": \"<RESOURCE-GROUP>\",\n",
    "        \"workspace_name\": \"<WORKSPACE-NAME>\"\n",
    "    }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Training Script\n",
    "For this tutorial, we will be using the *tf.keras module* to train a basic feed forward neural network on the IBM Employee Attrition Dataset. \n",
    "\n",
    "**We will start by writing the training cript into a train.py file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess_data(data):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Dropping Employee count as all values are 1 and hence attrition is independent of this feature\n",
    "    data = data.drop(['EmployeeCount'], axis=1)\n",
    "    \n",
    "    # Dropping Employee Number since it is merely an identifier\n",
    "    data = data.drop(['EmployeeNumber'], axis=1)\n",
    "    data = data.drop(['Over18'], axis=1)\n",
    "\n",
    "    # Since all values are 80\n",
    "    data = data.drop(['StandardHours'], axis=1)\n",
    "\n",
    "    # Converting target variables from string to numerical values\n",
    "    target_map = {'Yes': 1, 'No': 0}\n",
    "    data[\"Attrition_numerical\"] = data[\"Attrition\"].apply(lambda x: target_map[x])\n",
    "    target = data[\"Attrition_numerical\"]\n",
    "\n",
    "    data.drop(['Attrition_numerical', 'Attrition'], axis=1, inplace=True)\n",
    "    \n",
    "    # Creating dummy columns for each categorical feature\n",
    "    categorical = []\n",
    "    for col, value in data.iteritems():\n",
    "        if value.dtype == 'object':\n",
    "            categorical.append(col)\n",
    "\n",
    "    # Store the numerical columns in a list numerical\n",
    "    numerical = data.columns.difference(categorical)   \n",
    "\n",
    "    # We create the preprocessing pipelines for both numeric and categorical data.\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical),\n",
    "            ('cat', categorical_transformer, categorical)])\n",
    "    \n",
    "    pipeline = make_pipeline(preprocess)\n",
    "\n",
    "    # Split data into train and test sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, \n",
    "                                                        target, \n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=0,\n",
    "                                                        stratify=target)\n",
    "    \n",
    "    # Transform data\n",
    "    x_train_t = pd.DataFrame(pipeline.fit_transform(x_train))\n",
    "    x_test_t = pd.DataFrame(pipeline.transform(x_test))\n",
    "    \n",
    "    return x_train_t, x_test_t, y_train, y_test\n",
    "    \n",
    "# Load and preprocess data\n",
    "attrition_data = pd.read_csv('./data/data.csv')\n",
    "x_train, x_test, y_train, y_test = preprocess_data(attrition_data)\n",
    "\n",
    "# Create model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu', input_shape=(x_train.shape[1],)))\n",
    "model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy']) \n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train, epochs=20, verbose=1, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explain Model Locally\n",
    "\n",
    "We will start by training our model and explaining it in your local environment.\n",
    "\n",
    "**First step is to run the training script that we just wrote**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-10-27 15:58:36.700862: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2019-10-27 15:58:36.723711: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2112000000 Hz\n",
      "2019-10-27 15:58:36.728256: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fffd94ac210 executing computations on platform Host. Devices:\n",
      "2019-10-27 15:58:36.728766: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Host, Default Version\n",
      "WARNING:tensorflow:Falling back from v2 loop because of error: Failed to find data adapter that can handle input: <class 'pandas.core.frame.DataFrame'>, <class 'NoneType'>\n",
      "Train on 1176 samples, validate on 294 samples\n",
      "Epoch 1/20\n",
      "1176/1176 [==============================] - 0s 357us/sample - loss: 0.8064 - accuracy: 0.2959 - val_loss: 0.7107 - val_accuracy: 0.4898\n",
      "Epoch 2/20\n",
      "1176/1176 [==============================] - 0s 32us/sample - loss: 0.6746 - accuracy: 0.5561 - val_loss: 0.6247 - val_accuracy: 0.6837\n",
      "Epoch 3/20\n",
      "1176/1176 [==============================] - 0s 30us/sample - loss: 0.5971 - accuracy: 0.7611 - val_loss: 0.5630 - val_accuracy: 0.8129\n",
      "Epoch 4/20\n",
      "1176/1176 [==============================] - 0s 35us/sample - loss: 0.5401 - accuracy: 0.8172 - val_loss: 0.5155 - val_accuracy: 0.8367\n",
      "Epoch 5/20\n",
      "1176/1176 [==============================] - 0s 31us/sample - loss: 0.4979 - accuracy: 0.8350 - val_loss: 0.4812 - val_accuracy: 0.8435\n",
      "Epoch 6/20\n",
      "1176/1176 [==============================] - 0s 30us/sample - loss: 0.4674 - accuracy: 0.8410 - val_loss: 0.4581 - val_accuracy: 0.8401\n",
      "Epoch 7/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.4454 - accuracy: 0.8401 - val_loss: 0.4404 - val_accuracy: 0.8401\n",
      "Epoch 8/20\n",
      "1176/1176 [==============================] - 0s 30us/sample - loss: 0.4290 - accuracy: 0.8401 - val_loss: 0.4283 - val_accuracy: 0.8401\n",
      "Epoch 9/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.4154 - accuracy: 0.8410 - val_loss: 0.4180 - val_accuracy: 0.8401\n",
      "Epoch 10/20\n",
      "1176/1176 [==============================] - 0s 30us/sample - loss: 0.4036 - accuracy: 0.8410 - val_loss: 0.4096 - val_accuracy: 0.8401\n",
      "Epoch 11/20\n",
      "1176/1176 [==============================] - 0s 30us/sample - loss: 0.3935 - accuracy: 0.8418 - val_loss: 0.4032 - val_accuracy: 0.8435\n",
      "Epoch 12/20\n",
      "1176/1176 [==============================] - 0s 31us/sample - loss: 0.3844 - accuracy: 0.8435 - val_loss: 0.3970 - val_accuracy: 0.8469\n",
      "Epoch 13/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.3759 - accuracy: 0.8452 - val_loss: 0.3922 - val_accuracy: 0.8503\n",
      "Epoch 14/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.3685 - accuracy: 0.8478 - val_loss: 0.3888 - val_accuracy: 0.8503\n",
      "Epoch 15/20\n",
      "1176/1176 [==============================] - 0s 28us/sample - loss: 0.3612 - accuracy: 0.8520 - val_loss: 0.3850 - val_accuracy: 0.8537\n",
      "Epoch 16/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.3546 - accuracy: 0.8529 - val_loss: 0.3820 - val_accuracy: 0.8605\n",
      "Epoch 17/20\n",
      "1176/1176 [==============================] - 0s 27us/sample - loss: 0.3480 - accuracy: 0.8529 - val_loss: 0.3795 - val_accuracy: 0.8605\n",
      "Epoch 18/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.3411 - accuracy: 0.8588 - val_loss: 0.3760 - val_accuracy: 0.8605\n",
      "Epoch 19/20\n",
      "1176/1176 [==============================] - 0s 27us/sample - loss: 0.3353 - accuracy: 0.8614 - val_loss: 0.3736 - val_accuracy: 0.8639\n",
      "Epoch 20/20\n",
      "1176/1176 [==============================] - 0s 29us/sample - loss: 0.3293 - accuracy: 0.8631 - val_loss: 0.3704 - val_accuracy: 0.8605\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model and perform interpretability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING - From /anaconda/envs/azureml_py36/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# TODO:  LOAD MODEL AND EXPLAIN IT\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# from azureml.explain.model.tabular_explainer import TabularExplainer\n",
    "# # \"features\" and \"classes\" fields are optional\n",
    "# explainer = TabularExplainer(network, \n",
    "#                              train)\n",
    "\n",
    "# # you can use the training data or the test data here\n",
    "# global_explanation = explainer.explain_global(x_train)\n",
    "\n",
    "# # if you used the PFIExplainer in the previous step, use the next line of code instead\n",
    "# # global_explanation = explainer.explain_global(x_train, true_labels=y_test)\n",
    "\n",
    "# # sorted feature importance values and feature names\n",
    "# sorted_global_importance_values = global_explanation.get_ranked_global_values()\n",
    "# sorted_global_importance_names = global_explanation.get_ranked_global_names()\n",
    "# dict(zip(sorted_global_importance_names, sorted_global_importance_values))\n",
    "\n",
    "# # alternatively, you can print out a dictionary that holds the top K feature names and values\n",
    "# global_explanation.get_feature_importance_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Explain Locally\n",
    "We will start by training our model locally in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Explain Remotely\n",
    "Now we will train our model on the compute target created back in the [first tutorial]()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability In Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Feature Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
