{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing with TensorFlow 2.0 on Azure Machine Learning Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Workshop\n",
    "\n",
    "This notebook is Part 2 (Inferencing and Deploying a Model) of a four part workshop that demonstrates an end-to-end workflow for implementing a BERT model using Tensorflow 2.0 on Azure Machine Learning Service. The different components of the workshop are as follows:\n",
    "\n",
    "- Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb)\n",
    "- Part 2: [Inferencing and Deploying a Model](2_AzureServiceClassifier_Inferencing.ipynb)\n",
    "- Part 3: [Setting Up a Pipeline Using MLOps]()\n",
    "- Part 4: [Explaining Your Model Interoperability]()\n",
    "\n",
    "This workshop shows how to convert a TF 2.0 BERT model and deploy the model as Webservice in step-by-step fashion:\n",
    "\n",
    " 1. Initilize your workspace\n",
    " 2. Download a previous saved model (saved on Azure Machine Learning)\n",
    " 3. Test the downloaded model\n",
    " 6. Display scoring script\n",
    " 7. Defining an Azure Environment\n",
    " 8. Deploy Model as Webservice (Local, ACI and AKS)\n",
    " 9. Test Deployment (Azure ML Service Call, Raw HTTP Request)\n",
    " 10. Clean up Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure Service Classification Problem \n",
    "One of the key tasks to ensuring long term success of any Azure service is actively responding to related posts in online forums such as Stackoverflow. In order to keep track of these posts, Microsoft relies on the associated tags to direct questions to the appropriate support team. While Stackoverflow has different tags for each Azure service (azure-web-app-service, azure-virtual-machine-service, etc), people often use the generic **azure** tag. This makes it hard for specific teams to track down issues related to their product and as a result, many questions get left unanswered. \n",
    "\n",
    "In order to solve this problem, we will be building a model to classify Azure-related Stackoverflow posts with the appropriate Azure service tag. \n",
    "\n",
    "#### The Solution\n",
    "We will be using a BERT (Bidirectional Encoder Representations from Transformers) model which was published by researchers at Google AI Language. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of natural language processing (NLP) tasks without substantial architecture modifications.\n",
    "\n",
    "For more information about the BERT, please read this [paper](https://arxiv.org/pdf/1810.04805.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "* Understand the [architecture and terms](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture) introduced by Azure Machine Learning\n",
    "* If you are using an Azure Machine Learning Notebook VM, you are all set. Otherwise, go through the [configuration notebook](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-1st-experiment-sdk-setup) to:\n",
    "    * install the AML SDK\n",
    "    * create a workspace and its configuration file (config.json)\n",
    "* For local scoring test, you will also need to have Tensorflow and Keras installed in the current Jupyter kernel.\n",
    "* Please run through Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb) Notebook first to register your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.69\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Workspace\n",
    "\n",
    "Initialize a [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspace) object from the existing workspace you created in the Prerequisites step. Workspace.from_config() creates a workspace object from the details stored in config.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace name: abe-gpu-ws\n",
      "Azure region: westus2\n",
      "Subscription id: 15ae9cb6-95c1-483d-a0e3-b1a1a3b06324\n",
      "Resource group: osomorog\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registering the Model with the Workspace\n",
    "Get the trained model from an Azure Blob container, then register it to use in your workspace. The model is saved into two files, ``config.json`` and ``model.h5``. Azure ML automatically uploaded and is associated with the registered model. We can use the model object to download the model files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.datastore import Datastore\n",
    "\n",
    "datastore_name= \"exports\"\n",
    "container_name=\"azureml-blobstore-e32b7415-1cbf-4e53-9a40-99249b30c1a3\"\n",
    "account_name = 'abegpuwsstorage1730faf92'\n",
    "account_key = '<account_key>'\n",
    "model_datastore = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name, \n",
    "                      container_name= container_name, \n",
    "                      account_name=account_name, \n",
    "                      account_key=account_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_datastore.download('.',prefix=\"exports\")\n",
    "\n",
    "#register downloaded model\n",
    "model = Model.register(model_path = \"./exports\",\n",
    "                       model_name = \"azure-service-classifer\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"BERT\"},\n",
    "                       workspace = ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading and using registered models\n",
    "> If you already completed Part 1: [Working With Data and Training](1_AzureServiceClassifier_Training.ipynb) Notebook.You can dowload your registered BERT Model and use that instead of the model saved on the blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.model import Model\n",
    "\n",
    "# model = ws.models['azure-service-classifier']\n",
    "# model.download(target_dir='./exports', exist_ok=False, exists_ok=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict on the test set\n",
    "Let's check the version of the local Keras. Make sure it matches with the version number printed out in the training script. Otherwise you might not be able to load the model properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras version: 2.3.0\n",
      "Tensorflow version: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"Keras version:\", keras.__version__)\n",
    "print(\"Tensorflow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the BERT model\n",
    "Load the downloaded BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "class TFBertForMultiClassification(TFBertPreTrainedModel):\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultiClassification, self).__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(config.num_labels,\n",
    "                                                kernel_initializer=get_initializer(config.initializer_range),\n",
    "                                                name='classifier',\n",
    "                                                activation='softmax')\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "    \n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "model_dir = \"exports\"\n",
    "loaded_model = TFBertForMultiClassification.from_pretrained(model_dir, num_labels=len(labels))\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "print(\"Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed in test sentence to test the BERT model. And time the duration of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'azure-virtual-machine', 'probability': '0.964801'}\n",
      "CPU times: user 852 ms, sys: 44 ms, total: 896 ms\n",
      "Wall time: 282 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json \n",
    "\n",
    "# Input test sentences\n",
    "raw_data = json.dumps({\n",
    "    'text': 'VMs not working'\n",
    "})\n",
    "\n",
    " # Encode inputs using tokenizer\n",
    "inputs = tokenizer.encode_plus(\n",
    "    json.loads(raw_data)['text'],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_seq_length\n",
    "    )\n",
    "input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "padding_length = max_seq_length - len(input_ids)\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    # Make prediction\n",
    "predictions = loaded_model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor([attention_mask], dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor([token_type_ids], dtype=tf.int32)\n",
    "    })\n",
    "\n",
    "result =  {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see based on the sample sentence the model can predict the probablity of the stackover flow tags related to that sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing with ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ONNX (Open Neural Network Exchange)** is an interoperable standard format for ML models, with support for both DNN and traditional ML. Models can be converted from a variety of frameworks, such as Tensorflow, Keras, PyTorch, scikit-learn, and more (see [ONNX Conversion tutorials](https://github.com/onnx/tutorials#converting-to-onnx-format). This provides data teams with the flexibility to use their framework of choice for their training needs, while streamlining the process to operationalize these models for production usage in a consistent way.\n",
    "\n",
    "In this section, we will demonstrate how to use ONNX Runtime, a high performance inference engine for ONNX format models, for inferencing our model. Along with interoperability, ONNX Runtime's performance-focused architecture can also accelerate  inferencing for many models through graph optimizations, utilization of custom accelerators, and more. You can find more about performance tuning [here](https://github.com/microsoft/onnxruntime/blob/master/docs/ONNX_Runtime_Perf_Tuning.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ONNX Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONNX Model loaded from disk.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import onnxruntime as rt\n",
    "from transformers import BertTokenizer, TFBertPreTrainedModel, TFBertMainLayer\n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "sess = rt.InferenceSession(\"bert_tf2_new.onnx\")\n",
    "print(\"ONNX Model loaded from disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the inputs and outputs of converted ONNX model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input name  : token_type_ids:0\n",
      "Input shape : ['unk__847', 128]\n",
      "Input type  : tensor(int32)\n",
      "Input name  : input_ids:0\n",
      "Input shape : ['unk__848', 128]\n",
      "Input type  : tensor(int32)\n",
      "Input name  : attention_mask:0\n",
      "Input shape : ['unk__849', 128]\n",
      "Input type  : tensor(int32)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sess.get_inputs())):\n",
    "    input_name = sess.get_inputs()[i].name\n",
    "    print(\"Input name  :\", input_name)\n",
    "    input_shape = sess.get_inputs()[i].shape\n",
    "    print(\"Input shape :\", input_shape)\n",
    "    input_type = sess.get_inputs()[i].type\n",
    "    print(\"Input type  :\", input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output name  : tf_bert_for_multi_classification/Identity:0\n",
      "Output shape : ['unk__850', 5]\n",
      "Output type  : tensor(float)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sess.get_outputs())):\n",
    "    output_name = sess.get_outputs()[i].name\n",
    "    print(\"Output name  :\", output_name)  \n",
    "    output_shape = sess.get_outputs()[i].shape\n",
    "    print(\"Output shape :\", output_shape)\n",
    "    output_type = sess.get_outputs()[i].type\n",
    "    print(\"Output type  :\", output_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferencing with ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prediction': 'azure-virtual-machine', 'probability': '0.98157984'}\n",
      "CPU times: user 817 ms, sys: 0 ns, total: 817 ms\n",
      "Wall time: 212 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import json \n",
    "\n",
    "# Input test sentences\n",
    "raw_data = json.dumps({\n",
    "    'text': 'VM is not working'\n",
    "})\n",
    "\n",
    "labels = ['azure-web-app-service', 'azure-storage', 'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "\n",
    "# Encode inputs using tokenizer\n",
    "inputs = tokenizer.encode_plus(\n",
    "    json.loads(raw_data)['text'],\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_seq_length\n",
    "    )\n",
    "input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n",
    "attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "padding_length = max_seq_length - len(input_ids)\n",
    "input_ids = input_ids + ([0] * padding_length)\n",
    "attention_mask = attention_mask + ([0] * padding_length)\n",
    "token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "    \n",
    "    # Make prediction\n",
    "convert_input = {\n",
    "        sess.get_inputs()[0].name: np.array(tf.convert_to_tensor([token_type_ids], dtype=tf.int32)),\n",
    "        sess.get_inputs()[1].name: np.array(tf.convert_to_tensor([input_ids], dtype=tf.int32)),\n",
    "        sess.get_inputs()[2].name: np.array(tf.convert_to_tensor([attention_mask], dtype=tf.int32))\n",
    "    }\n",
    "\n",
    "predictions = sess.run([output_name], convert_input)\n",
    "\n",
    "result =  {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as web service\n",
    "\n",
    "Now we are ready to deploy the model as a web service running on your [local](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where#local) machine, in Azure Container Instance [ACI](https://azure.microsoft.com/en-us/services/container-instances/) or Azure Kubernetes Service [AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/). Azure Machine Learning accomplishes this by constructing a Docker image with the scoring logic and model baked in. \n",
    "\n",
    "Once you've tested the model and are satisfied with the results, deploy the model as a web service. For this Notebook, we'll use the original model format for deployment, but note that the ONNX model can be deployed in the same way by using ONNX Runtime in the scoring script.\n",
    "\n",
    "To build the correct environment, provide the following:\n",
    "* A scoring script to show how to use the model\n",
    "* An environment file to show what packages need to be installed\n",
    "* A configuration file to build the web service\n",
    "* The model you trained before\n",
    "\n",
    "Read more about deployment [here](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-deploy-and-where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will create a scoring script that will be invoked by the web service call.\n",
    "\n",
    "* Note that the scoring script must have two required functions, init() and run(input_data).\n",
    "    * In init() function, you typically load the model into a global object. This function is executed only once when the Docker container is started.\n",
    "    * In run(input_data) function, the model is used to predict a value based on the input data. The input and output to run typically use JSON as serialization and de-serialization format but you are not limited to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat code/scoring/score.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now create and/or use an Environment object when deploying a Webservice. The Environment can have been previously registered with your Workspace, or it will be registered with it as a part of the Webservice deployment. Only Environments that were created using azureml-defaults version 1.0.48 or later will work with this new handling however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "\n",
    "myenv = CondaDependencies.create(conda_packages=['numpy','pandas'],\n",
    "                                 pip_packages=['numpy','pandas','inference-schema[numpy-support]','azureml-defaults','tensorflow==2.0.0','transformers==2.0.0'])\n",
    "\n",
    "with open(\"code/scoring/myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the content of the `myenv.yml` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m# Conda environment specification. The dependencies defined in this file will\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;31m# be automatically provisioned for runs with userManagedDependencies=False.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;31m# Details about the Conda environment file format:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;31m# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mproject_environment\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0mdependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;31m# The python interpreter version.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;31m# Currently Azure ML only supports 3.5.2 and later.\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpython\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3.6\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;36m.69\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2.0\u001b[0m\u001b[0;36m.0\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mconda\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mforge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pycat code/scoring/myenv.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Inference Configuration\n",
    "\n",
    "There is now support for a source directory, you can upload an entire folder from your local machine as dependencies for the Webservice.\n",
    "Note: in that case, your entry_script and conda_file paths are relative paths to the source_directory path.\n",
    "\n",
    "Sample code for using a source directory:\n",
    "\n",
    "```python\n",
    "inference_config = InferenceConfig(source_directory=\"C:/abc\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"x/y/score.py\",\n",
    "                                   conda_file=\"env/myenv.yml\")\n",
    "```\n",
    "\n",
    " - source_directory = holds source path as string, this entire folder gets added in image so its really easy to access any files within this folder or subfolder\n",
    " - runtime = Which runtime to use for the image. Current supported runtimes are 'spark-py' and 'python\n",
    " - entry_script = contains logic specific to initializing your model and running predictions\n",
    " - conda_file = manages conda and python package dependencies.\n",
    " \n",
    " > **Note:** Deployment uses the inference configuration deployment configuration to deploy the models. The deployment process is similar regardless of the compute target. Deploying to AKS is slightly different because you must provide a reference to the AKS cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "inference_config = InferenceConfig(source_directory=\"code/scoring\",\n",
    "                                   runtime= \"python\", \n",
    "                                   entry_script=\"score.py\",\n",
    "                                   conda_file=\"myenv.yml\"\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy as a Local Service\n",
    "\n",
    "Estimated time to complete: **about 3-7 minutes**\n",
    "\n",
    "Configure the image and deploy it locally. The following code goes through these steps:\n",
    "\n",
    "* Build an image on local machine (or VM, if you are using a VM) using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file \n",
    "* Send the image to local docker instance.\n",
    "* Start up a container using the image.\n",
    "* Get the web service HTTP endpoint.\n",
    "* This has a very quick turnaround time and is great for testing service before it is deployed to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import InferenceConfig, Model\n",
    "from azureml.core.webservice import LocalWebservice\n",
    "\n",
    "# Create a local deployment for the web service endpoint\n",
    "deployment_config = LocalWebservice.deploy_configuration()\n",
    "# Deploy the service\n",
    "local_service = Model.deploy(\n",
    "    ws, \"mymodel\", [model], inference_config, deployment_config)\n",
    "# Wait for the deployment to complete\n",
    "local_service.wait_for_deployment(True)\n",
    "# Display the port that the web service is available on\n",
    "print(local_service.port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(local_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Local Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the deployed model. Pick a random samples about an issue, and send it to the web service. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = local_service.run(input_data=raw_data)\n",
    "print(prediction)\n",
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reloading Webservice\n",
    "You can update your score.py file and then call reload() to quickly restart the service. This will only reload your execution script and dependency files, it will not rebuild the underlying Docker image. As a result, reload() is fast, but if you do need to rebuild the image -- to add a new Conda or pip package, for instance -- you will have to call update(), instead (see below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/scoring/score.py\n",
    "import os\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertPreTrainedModel, TFBertMainLayer, BertTokenizer\n",
    "from transformers.modeling_tf_utils import get_initializer\n",
    "import logging\n",
    "logging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "class TFBertForMultiClassification(TFBertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, *inputs, **kwargs):\n",
    "        super(TFBertForMultiClassification, self) \\\n",
    "            .__init__(config, *inputs, **kwargs)\n",
    "        self.num_labels = config.num_labels\n",
    "\n",
    "        self.bert = TFBertMainLayer(config, name='bert')\n",
    "        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(\n",
    "            config.num_labels,\n",
    "            kernel_initializer=get_initializer(config.initializer_range),\n",
    "            name='classifier',\n",
    "            activation='softmax')\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        outputs = self.bert(inputs, **kwargs)\n",
    "\n",
    "        pooled_output = outputs[1]\n",
    "\n",
    "        pooled_output = self.dropout(\n",
    "            pooled_output,\n",
    "            training=kwargs.get('training', False))\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        # add hidden states and attention if they are here\n",
    "        outputs = (logits,) + outputs[2:]\n",
    "\n",
    "        return outputs  # logits, (hidden_states), (attentions)\n",
    "\n",
    "\n",
    "max_seq_length = 128\n",
    "labels = ['azure-web-app-service', 'azure-storage',\n",
    "    'azure-devops', 'azure-virtual-machine', 'azure-functions']\n",
    "\n",
    "\n",
    "def init():\n",
    "    global tokenizer, model\n",
    "    # os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'azure-service-classifier')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model_dir = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'exports')\n",
    "    model = TFBertForMultiClassification \\\n",
    "        .from_pretrained(model_dir, num_labels=len(labels))\n",
    "    print(\"hello from the reloaded script\")\n",
    "\n",
    "def run(raw_data):\n",
    "\n",
    "    # Encode inputs using tokenizer\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        json.loads(raw_data)['text'],\n",
    "        add_special_tokens=True,\n",
    "        max_length=max_seq_length\n",
    "    )\n",
    "    input_ids, token_type_ids = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens.\n",
    "    # Only real tokens are attended to.\n",
    "    attention_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    input_ids = input_ids + ([0] * padding_length)\n",
    "    attention_mask = attention_mask + ([0] * padding_length)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_length)\n",
    "\n",
    "    # Make prediction\n",
    "    predictions = model.predict({\n",
    "        'input_ids': tf.convert_to_tensor([input_ids], dtype=tf.int32),\n",
    "        'attention_mask': tf.convert_to_tensor(\n",
    "            [attention_mask],\n",
    "            dtype=tf.int32),\n",
    "        'token_type_ids': tf.convert_to_tensor(\n",
    "            [token_type_ids], \n",
    "            dtype=tf.int32)\n",
    "    })\n",
    "\n",
    "    result = {\n",
    "        'prediction': str(labels[predictions[0].argmax().item()]),\n",
    "        'probability': str(predictions[0].max())\n",
    "    }\n",
    "\n",
    "    print(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "init()\n",
    "run(json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.reload()\n",
    "\n",
    "## Update Service\n",
    "#local_service.update(models=[loaded_model], image_config=None, deployment_config=None, wait=False, inference_config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(local_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy in ACI\n",
    "Estimated time to complete: **about 3-7 minutes**\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "* Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "* Send the image to the ACI container.\n",
    "* Start up a container in ACI using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.exceptions import WebserviceException\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "## Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your ACI container. \n",
    "## If you feel you need more later, you would have to recreate the image and redeploy the service.\n",
    "aciconfig = AciWebservice.deploy_configuration(cpu_cores=2, \n",
    "                                               memory_gb=4, \n",
    "                                               tags={\"model\": \"BERT\",  \"method\" : \"tensorflow\"}, \n",
    "                                               description='Predict StackoverFlow tags with BERT')\n",
    "\n",
    "aci_service_name = 'asc-aciservice'\n",
    "\n",
    "try:\n",
    "    # if you want to get existing service below is the command\n",
    "    # since aci name needs to be unique in subscription deleting existing aci if any\n",
    "    # we use aci_service_name to create azure ac\n",
    "    aci_service = Webservice(ws, name=aci_service_name)\n",
    "    if aci_service:\n",
    "        aci_service.delete()\n",
    "except WebserviceException as e:\n",
    "    print()\n",
    "\n",
    "aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n",
    "\n",
    "aci_service.wait_for_deployment(True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the scoring web service endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aci_service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the deployed model. Pick a random samples about an Azure issue, and send it to the web service. Note here we are using the run API in the SDK to invoke the service. You can also make raw HTTP calls using any HTTP tool such as curl.\n",
    "\n",
    "After the invocation, we print the returned predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = aci_service.run(input_data=raw_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(aci_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy in AKS (Single Node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time to complete: **about 15-25 minutes**, 10-15 mins for AKS provisioning and 5-10 mins to deploy service\n",
    "\n",
    "Configure the image and deploy. The following code goes through these steps:\n",
    "\n",
    "* Provision a Dev Test AKS Cluster\n",
    "* Build an image using:\n",
    "   * The scoring file (`score.py`)\n",
    "   * The environment file (`myenv.yml`)\n",
    "   * The model file\n",
    "* Send the image to the AKS cluster.\n",
    "* Start up a container in AKS using the image.\n",
    "* Get the web service HTTP endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provisioning Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AksCompute, ComputeTarget\n",
    "\n",
    "# Use the default configuration (you can also provide parameters to customize this).\n",
    "# For example, to create a dev/test cluster, use:\n",
    "# prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "prov_config = AksCompute.provisioning_configuration(cluster_purpose = AksCompute.ClusterPurpose.DEV_TEST)\n",
    "\n",
    "aks_name = 'myaks'\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws,\n",
    "                                    name = aks_name,\n",
    "                                    provisioning_configuration = prov_config)\n",
    "\n",
    "# Wait for the create process to complete\n",
    "aks_target.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AksWebservice, Webservice\n",
    "from azureml.core.model import Model\n",
    "\n",
    "aks_target = AksCompute(ws,\"myaks\")\n",
    "\n",
    "## Create a deployment configuration file and specify the number of CPUs and gigabyte of RAM needed for your cluster. \n",
    "## If you feel you need more later, you would have to recreate the image and redeploy the service.\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 2, memory_gb = 4)\n",
    "\n",
    "aks_service = Model.deploy(ws, \"myservice\", [model], inference_config, deployment_config, aks_target)\n",
    "aks_service.wait_for_deployment(show_output = True)\n",
    "print(aks_service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Azure SDK service call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Azure SDK to make a service call with a simple function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "raw_data = json.dumps({\n",
    "    'text': 'My VM is not working'\n",
    "})\n",
    "\n",
    "prediction = aks_service.run(input_data=raw_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using HTTP call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now send construct raw HTTP request and send to the service. Don't forget to add key to the HTTP header."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install Jupyter Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Web Service with HTTP call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import Layout, Button, Box, FloatText, Textarea, Dropdown, Label, IntSlider, VBox\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "text = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Type a query',\n",
    "    description='Question:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "button = widgets.Button(description=\"Get Tag!\")\n",
    "output = widgets.Output()\n",
    "\n",
    "items = [text, button] \n",
    "\n",
    "box_layout = Layout(display='flex',\n",
    "                    flex_flow='row',\n",
    "                    align_items='stretch',\n",
    "                    width='70%')\n",
    "\n",
    "box_auto = Box(children=items, layout=box_layout)\n",
    "\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        input_data = '{\\\"text\\\": \\\"'+ text.value +'\\\"}'\n",
    "        headers = {'Content-Type':'application/json'}\n",
    "        resp = requests.post(local_service.scoring_uri, input_data, headers=headers)\n",
    "       \n",
    "        print(\"=\"*10)\n",
    "        print(\"Question:\", text.value)\n",
    "        print(\"POST to url\", local_service.scoring_uri)\n",
    "        print(\"Prediction:\", resp.text)\n",
    "        print(\"=\"*10)\n",
    "\n",
    "button.on_click(on_button_clicked)\n",
    "\n",
    "#Display the GUI\n",
    "VBox([box_auto, output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View service Logs (Debug, when something goes wrong )\n",
    "**Tip: If something goes wrong with the deployment, the first thing to look at is the logs from the service by running the following command:** Run this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "pp.pprint(aks_service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of workspace\n",
    "Let's look at the workspace after the web service was deployed. You should see\n",
    "\n",
    "* a registered model named and with the id \n",
    "* an AKS and ACI webservice called with some scoring URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ws.models\n",
    "for name, model in models.items():\n",
    "    print(\"Model: {}, ID: {}\".format(name, model.id))\n",
    "    \n",
    "webservices = ws.webservices\n",
    "for name, webservice in webservices.items():\n",
    "    print(\"Webservice: {}, scoring URI: {}\".format(name, webservice.scoring_uri))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete ACI to clean up\n",
    "You can delete the ACI deployment with a simple delete API call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_service.delete()\n",
    "aci_service.delete()\n",
    "aks_service.delete()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (aml-hack)",
   "language": "python",
   "name": "aml-hack"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
